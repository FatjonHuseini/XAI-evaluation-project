{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Original Weights LRP.ipynb","provenance":[{"file_id":"11Rcjfv1LWVoe77IqcKA3AhAhdXaQCmCY","timestamp":1654782733795},{"file_id":"1qu0RpAJwxjnP12n5WYEGQIb7rWP0YERB","timestamp":1654687424609},{"file_id":"10N73fDLIu7yF_sXd8e7f7owHhL0Kg11_","timestamp":1654286404313}],"machine_shape":"hm","collapsed_sections":[],"mount_file_id":"1KynUEmuqkQfEmBGcLLWcU2Yge2j6dHgf","authorship_tag":"ABX9TyOPHbAXRr9jWtPCOOOaf9ni"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f82aaaef643e45abb0db71114ef4be24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6316523a8eb44508965d274422027452","IPY_MODEL_9e2a10880d2c4cefaf2c1339c35973a7","IPY_MODEL_d5fac6c66a894d3d927b74f52e477a15"],"layout":"IPY_MODEL_fa3bae39e45d4a0a867d57f33a21b37f"}},"6316523a8eb44508965d274422027452":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_033573910b5345e7afbed0368e446d4d","placeholder":"​","style":"IPY_MODEL_1a1270a28bdf48549a7985761d68b48d","value":"Downloading: 100%"}},"9e2a10880d2c4cefaf2c1339c35973a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72c463cead194382ad9499ce239b8c79","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6dfadec1f494b978668df8707533c84","value":231508}},"d5fac6c66a894d3d927b74f52e477a15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc834b832bf640a1816f05c87eee79cd","placeholder":"​","style":"IPY_MODEL_78d140183599429fb009b620e89550cc","value":" 226k/226k [00:00&lt;00:00, 817kB/s]"}},"fa3bae39e45d4a0a867d57f33a21b37f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"033573910b5345e7afbed0368e446d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a1270a28bdf48549a7985761d68b48d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72c463cead194382ad9499ce239b8c79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6dfadec1f494b978668df8707533c84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc834b832bf640a1816f05c87eee79cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78d140183599429fb009b620e89550cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a64326cb2e0f46019a8c0e83b4c0e567":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d39cce0b551b444996f4d4fd3ebdf193","IPY_MODEL_23bddbea4d474129be634d109338f628","IPY_MODEL_caf117c105bb4604abb9e0e6175218e4"],"layout":"IPY_MODEL_919288a6beb7443fb76d8882f1cca753"}},"d39cce0b551b444996f4d4fd3ebdf193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6b6a236589a43a5bf7e01f0ebc2357d","placeholder":"​","style":"IPY_MODEL_3f2ba90b63514185970fb832c4f663d2","value":"Downloading: 100%"}},"23bddbea4d474129be634d109338f628":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d8854be22544adeb34a8f670747eb0f","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22f2aeb26c634367a05867699f3cb753","value":28}},"caf117c105bb4604abb9e0e6175218e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d3d237b69ad479eb5ec52f2d88e3b4a","placeholder":"​","style":"IPY_MODEL_4b7d5197d69641e9b73125c6fdb4b6ef","value":" 28.0/28.0 [00:00&lt;00:00, 445B/s]"}},"919288a6beb7443fb76d8882f1cca753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6b6a236589a43a5bf7e01f0ebc2357d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f2ba90b63514185970fb832c4f663d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d8854be22544adeb34a8f670747eb0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f2aeb26c634367a05867699f3cb753":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d3d237b69ad479eb5ec52f2d88e3b4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b7d5197d69641e9b73125c6fdb4b6ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c61ccfe58f3f446f90db3867463f60ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7c578d4f0c340c38142ac3b7e9373bc","IPY_MODEL_c952559805424d249e120049acdd862f","IPY_MODEL_9b95f7a8bc0f4b66b9e4aa6a37218fa4"],"layout":"IPY_MODEL_fc352138791c40ecb5842729374b8e3e"}},"b7c578d4f0c340c38142ac3b7e9373bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48238e742cf44393839ddf565fdeb33d","placeholder":"​","style":"IPY_MODEL_3561260cf61c4da097793277feb905d6","value":"Downloading: 100%"}},"c952559805424d249e120049acdd862f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f52838aeef846918c6e8e12e21d55b3","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c93e138fcf84fa6beecd0b938bd44b9","value":570}},"9b95f7a8bc0f4b66b9e4aa6a37218fa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a1542515b1148f6bd49586df2a28153","placeholder":"​","style":"IPY_MODEL_7f86bba94cc84c8ab29c77b6da6cb499","value":" 570/570 [00:00&lt;00:00, 15.5kB/s]"}},"fc352138791c40ecb5842729374b8e3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48238e742cf44393839ddf565fdeb33d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3561260cf61c4da097793277feb905d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f52838aeef846918c6e8e12e21d55b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c93e138fcf84fa6beecd0b938bd44b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a1542515b1148f6bd49586df2a28153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f86bba94cc84c8ab29c77b6da6cb499":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9bf8d29ebf745b8b3fa5bb990e25bea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f6b4e5f77e945de8e90dc0f15803ea6","IPY_MODEL_25147f6c3bf74bdc94a070ab7aec2a3e","IPY_MODEL_35c98b6ef2754850963b098e28893e75"],"layout":"IPY_MODEL_a2b187d88e12400a9c7936807e864dc4"}},"2f6b4e5f77e945de8e90dc0f15803ea6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed51a0aeae7d4ec3ae6484e3e8d59b52","placeholder":"​","style":"IPY_MODEL_8ea7df9efae24376aa9ef46ae3bd40fa","value":"Downloading: 100%"}},"25147f6c3bf74bdc94a070ab7aec2a3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcdc0abeb5c74c57bc9f9f5baee9a020","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14e4639375e64065be062e958f91e02f","value":440473133}},"35c98b6ef2754850963b098e28893e75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed8c905a84e242e1885892ad149aed0c","placeholder":"​","style":"IPY_MODEL_5e00fabd1a074cfa84fbfb765f2ca076","value":" 420M/420M [00:14&lt;00:00, 35.7MB/s]"}},"a2b187d88e12400a9c7936807e864dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed51a0aeae7d4ec3ae6484e3e8d59b52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea7df9efae24376aa9ef46ae3bd40fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcdc0abeb5c74c57bc9f9f5baee9a020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14e4639375e64065be062e958f91e02f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed8c905a84e242e1885892ad149aed0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e00fabd1a074cfa84fbfb765f2ca076":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install torchtext==0.4"],"metadata":{"id":"YN4QUx7jKB8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0Z8D74aHKXIu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654782780149,"user_tz":-120,"elapsed":10930,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"8999e931-948d-44cf-cf44-4c1ba539d00e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 29.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svP-LkLiJ1bK","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f82aaaef643e45abb0db71114ef4be24","6316523a8eb44508965d274422027452","9e2a10880d2c4cefaf2c1339c35973a7","d5fac6c66a894d3d927b74f52e477a15","fa3bae39e45d4a0a867d57f33a21b37f","033573910b5345e7afbed0368e446d4d","1a1270a28bdf48549a7985761d68b48d","72c463cead194382ad9499ce239b8c79","d6dfadec1f494b978668df8707533c84","cc834b832bf640a1816f05c87eee79cd","78d140183599429fb009b620e89550cc","a64326cb2e0f46019a8c0e83b4c0e567","d39cce0b551b444996f4d4fd3ebdf193","23bddbea4d474129be634d109338f628","caf117c105bb4604abb9e0e6175218e4","919288a6beb7443fb76d8882f1cca753","f6b6a236589a43a5bf7e01f0ebc2357d","3f2ba90b63514185970fb832c4f663d2","6d8854be22544adeb34a8f670747eb0f","22f2aeb26c634367a05867699f3cb753","3d3d237b69ad479eb5ec52f2d88e3b4a","4b7d5197d69641e9b73125c6fdb4b6ef","c61ccfe58f3f446f90db3867463f60ec","b7c578d4f0c340c38142ac3b7e9373bc","c952559805424d249e120049acdd862f","9b95f7a8bc0f4b66b9e4aa6a37218fa4","fc352138791c40ecb5842729374b8e3e","48238e742cf44393839ddf565fdeb33d","3561260cf61c4da097793277feb905d6","7f52838aeef846918c6e8e12e21d55b3","6c93e138fcf84fa6beecd0b938bd44b9","2a1542515b1148f6bd49586df2a28153","7f86bba94cc84c8ab29c77b6da6cb499","d9bf8d29ebf745b8b3fa5bb990e25bea","2f6b4e5f77e945de8e90dc0f15803ea6","25147f6c3bf74bdc94a070ab7aec2a3e","35c98b6ef2754850963b098e28893e75","a2b187d88e12400a9c7936807e864dc4","ed51a0aeae7d4ec3ae6484e3e8d59b52","8ea7df9efae24376aa9ef46ae3bd40fa","bcdc0abeb5c74c57bc9f9f5baee9a020","14e4639375e64065be062e958f91e02f","ed8c905a84e242e1885892ad149aed0c","5e00fabd1a074cfa84fbfb765f2ca076"]},"executionInfo":{"status":"ok","timestamp":1654782935727,"user_tz":-120,"elapsed":155583,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"f74103b3-69dd-47c4-d0ac-3764fbab59bc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f82aaaef643e45abb0db71114ef4be24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a64326cb2e0f46019a8c0e83b4c0e567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61ccfe58f3f446f90db3867463f60ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9bf8d29ebf745b8b3fa5bb990e25bea"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["downloading trainDevTestTrees_PTB.zip\n"]},{"output_type":"stream","name":"stderr","text":["trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:00<00:00, 3.78MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["extracting\n","Epoch 1\n","Batch 1: 12/32 correct (37.5%); loss = 1.2674005031585693\n","Batch 2: 14/32 correct (43.8%); loss = 1.2729907035827637\n","Batch 3: 13/32 correct (40.6%); loss = 1.2099485397338867\n","Batch 4: 14/32 correct (43.8%); loss = 1.1489810943603516\n","Batch 5: 19/32 correct (59.4%); loss = 1.0409433841705322\n","Batch 6: 11/32 correct (34.4%); loss = 1.333648443222046\n","Batch 7: 6/32 correct (18.8%); loss = 1.2459869384765625\n","Batch 8: 17/32 correct (53.1%); loss = 1.1076899766921997\n","Batch 9: 10/32 correct (31.2%); loss = 1.1544564962387085\n","Batch 10: 15/32 correct (46.9%); loss = 1.105334758758545\n","Batch 11: 14/32 correct (43.8%); loss = 1.1132664680480957\n","Batch 12: 13/32 correct (40.6%); loss = 1.0245308876037598\n","Batch 13: 14/32 correct (43.8%); loss = 1.086475133895874\n","Batch 14: 13/32 correct (40.6%); loss = 1.0844624042510986\n","Batch 15: 14/32 correct (43.8%); loss = 1.0857409238815308\n","Batch 16: 12/32 correct (37.5%); loss = 1.2320913076400757\n","Batch 17: 20/32 correct (62.5%); loss = 0.9100726842880249\n","Batch 18: 16/32 correct (50.0%); loss = 1.028321623802185\n","Batch 19: 19/32 correct (59.4%); loss = 1.126343846321106\n","Batch 20: 17/32 correct (53.1%); loss = 1.0194977521896362\n","Batch 21: 19/32 correct (59.4%); loss = 0.9932264089584351\n","Batch 22: 17/32 correct (53.1%); loss = 1.024118185043335\n","Batch 23: 19/32 correct (59.4%); loss = 1.0836421251296997\n","Batch 24: 25/32 correct (78.1%); loss = 0.8760244846343994\n","Batch 25: 19/32 correct (59.4%); loss = 1.0076022148132324\n","Batch 26: 15/32 correct (46.9%); loss = 1.0682157278060913\n","Batch 27: 20/32 correct (62.5%); loss = 0.98284512758255\n","Batch 28: 22/32 correct (68.8%); loss = 0.848297655582428\n","Batch 29: 18/32 correct (56.2%); loss = 0.9863288402557373\n","Batch 30: 23/32 correct (71.9%); loss = 0.8434063792228699\n","Batch 31: 20/32 correct (62.5%); loss = 0.9589886665344238\n","Batch 32: 24/32 correct (75.0%); loss = 0.7601900100708008\n","Batch 33: 25/32 correct (78.1%); loss = 0.7780817747116089\n","Batch 34: 23/32 correct (71.9%); loss = 0.7605041265487671\n","Batch 35: 21/32 correct (65.6%); loss = 0.816286027431488\n","Batch 36: 22/32 correct (68.8%); loss = 0.7965031266212463\n","Batch 37: 24/32 correct (75.0%); loss = 0.7754333019256592\n","Batch 38: 21/32 correct (65.6%); loss = 0.7689419984817505\n","Batch 39: 22/32 correct (68.8%); loss = 0.8259803056716919\n","Batch 40: 21/32 correct (65.6%); loss = 0.8634368777275085\n","Batch 41: 24/32 correct (75.0%); loss = 0.706684947013855\n","Batch 42: 21/32 correct (65.6%); loss = 0.8203081488609314\n","Batch 43: 22/32 correct (68.8%); loss = 0.7914111614227295\n","Batch 44: 21/32 correct (65.6%); loss = 0.9003753662109375\n","Batch 45: 23/32 correct (71.9%); loss = 0.6933144927024841\n","Batch 46: 26/32 correct (81.2%); loss = 0.5496256351470947\n","Batch 47: 22/32 correct (68.8%); loss = 0.7892996072769165\n","Batch 48: 21/32 correct (65.6%); loss = 0.889973521232605\n","Batch 49: 18/32 correct (56.2%); loss = 0.9089433550834656\n","Batch 50: 21/32 correct (65.6%); loss = 0.7520755529403687\n","Batch 51: 21/32 correct (65.6%); loss = 0.706335186958313\n","Batch 52: 23/32 correct (71.9%); loss = 0.6686708331108093\n","Batch 53: 20/32 correct (62.5%); loss = 0.9360923767089844\n","Batch 54: 20/32 correct (62.5%); loss = 1.1254254579544067\n","Batch 55: 21/32 correct (65.6%); loss = 0.7415727376937866\n","Batch 56: 22/32 correct (68.8%); loss = 0.7227643728256226\n","Batch 57: 26/32 correct (81.2%); loss = 0.6182618737220764\n","Batch 58: 23/32 correct (71.9%); loss = 0.775481104850769\n","Batch 59: 18/32 correct (56.2%); loss = 0.8820804953575134\n","Batch 60: 22/32 correct (68.8%); loss = 0.7690544128417969\n","Batch 61: 26/32 correct (81.2%); loss = 0.5184025764465332\n","Batch 62: 16/32 correct (50.0%); loss = 1.1081793308258057\n","Batch 63: 23/32 correct (71.9%); loss = 0.7736299633979797\n","Batch 64: 20/32 correct (62.5%); loss = 0.7948710322380066\n","Batch 65: 24/32 correct (75.0%); loss = 0.6543306112289429\n","Batch 66: 21/32 correct (65.6%); loss = 0.7918708920478821\n","Batch 67: 21/32 correct (65.6%); loss = 0.6641172766685486\n","Batch 68: 24/32 correct (75.0%); loss = 0.6758466362953186\n","Batch 69: 20/32 correct (62.5%); loss = 0.9798781275749207\n","Batch 70: 21/32 correct (65.6%); loss = 0.8829540610313416\n","Batch 71: 22/32 correct (68.8%); loss = 0.6846963763237\n","Batch 72: 21/32 correct (65.6%); loss = 0.7184485197067261\n","Batch 73: 26/32 correct (81.2%); loss = 0.63361656665802\n","Batch 74: 27/32 correct (84.4%); loss = 0.5132867693901062\n","Batch 75: 25/32 correct (78.1%); loss = 0.5671489238739014\n","Batch 76: 21/32 correct (65.6%); loss = 0.9526288509368896\n","Batch 77: 21/32 correct (65.6%); loss = 0.7479743361473083\n","Batch 78: 24/32 correct (75.0%); loss = 0.7563381791114807\n","Batch 79: 15/32 correct (46.9%); loss = 1.0234788656234741\n","Batch 80: 21/32 correct (65.6%); loss = 0.9086028337478638\n","Batch 81: 16/32 correct (50.0%); loss = 1.0839630365371704\n","Batch 82: 22/32 correct (68.8%); loss = 0.6300050616264343\n","Batch 83: 22/32 correct (68.8%); loss = 0.7554298043251038\n","Batch 84: 16/32 correct (50.0%); loss = 1.0203628540039062\n","Batch 85: 23/32 correct (71.9%); loss = 0.7035717368125916\n","Batch 86: 23/32 correct (71.9%); loss = 0.6115714311599731\n","Batch 87: 21/32 correct (65.6%); loss = 0.7027661800384521\n","Batch 88: 24/32 correct (75.0%); loss = 0.5956347584724426\n","Batch 89: 19/32 correct (59.4%); loss = 0.757561981678009\n","Batch 90: 22/32 correct (68.8%); loss = 0.8609920144081116\n","Batch 91: 26/32 correct (81.2%); loss = 0.6538965106010437\n","Batch 92: 24/32 correct (75.0%); loss = 0.7101671695709229\n","Batch 93: 21/32 correct (65.6%); loss = 0.8367573022842407\n","Batch 94: 21/32 correct (65.6%); loss = 0.697307288646698\n","Batch 95: 24/32 correct (75.0%); loss = 0.681829035282135\n","Batch 96: 25/32 correct (78.1%); loss = 0.7854332327842712\n","Batch 97: 26/32 correct (81.2%); loss = 0.5870999097824097\n","Batch 98: 21/32 correct (65.6%); loss = 0.8226103782653809\n","Batch 99: 20/32 correct (62.5%); loss = 0.8282721638679504\n","Batch 100: 23/32 correct (71.9%); loss = 0.6789937019348145\n","Batch 101: 20/32 correct (62.5%); loss = 0.8781662583351135\n","Batch 102: 24/32 correct (75.0%); loss = 0.6276260614395142\n","Batch 103: 20/32 correct (62.5%); loss = 0.8552145957946777\n","Batch 104: 21/32 correct (65.6%); loss = 0.8240736126899719\n","Batch 105: 22/32 correct (68.8%); loss = 0.7688941359519958\n","Batch 106: 18/32 correct (56.2%); loss = 0.9646624326705933\n","Batch 107: 25/32 correct (78.1%); loss = 0.5721712708473206\n","Batch 108: 22/32 correct (68.8%); loss = 0.7766891717910767\n","Batch 109: 26/32 correct (81.2%); loss = 0.5204702019691467\n","Batch 110: 25/32 correct (78.1%); loss = 0.6171132326126099\n","Batch 111: 23/32 correct (71.9%); loss = 0.6973896026611328\n","Batch 112: 23/32 correct (71.9%); loss = 0.6965649724006653\n","Batch 113: 23/32 correct (71.9%); loss = 0.6811904311180115\n","Batch 114: 24/32 correct (75.0%); loss = 0.6731066107749939\n","Batch 115: 21/32 correct (65.6%); loss = 0.7883192300796509\n","Batch 116: 22/32 correct (68.8%); loss = 0.7505098581314087\n","Batch 117: 25/32 correct (78.1%); loss = 0.5881778597831726\n","Batch 118: 19/32 correct (59.4%); loss = 1.0495051145553589\n","Batch 119: 21/32 correct (65.6%); loss = 0.8760837912559509\n","Batch 120: 25/32 correct (78.1%); loss = 0.6376373767852783\n","Batch 121: 17/32 correct (53.1%); loss = 1.0004596710205078\n","Batch 122: 21/32 correct (65.6%); loss = 0.8597186803817749\n","Batch 123: 26/32 correct (81.2%); loss = 0.5077412128448486\n","Batch 124: 21/32 correct (65.6%); loss = 0.8816741108894348\n","Batch 125: 22/32 correct (68.8%); loss = 0.7181260585784912\n","Batch 126: 21/32 correct (65.6%); loss = 0.7212562561035156\n","Batch 127: 24/32 correct (75.0%); loss = 0.615246593952179\n","Batch 128: 24/32 correct (75.0%); loss = 0.5821020603179932\n","Batch 129: 25/32 correct (78.1%); loss = 0.6201332807540894\n","Batch 130: 24/32 correct (75.0%); loss = 0.6400140523910522\n","Batch 131: 23/32 correct (71.9%); loss = 0.6199069619178772\n","Batch 132: 23/32 correct (71.9%); loss = 0.7126750946044922\n","Batch 133: 25/32 correct (78.1%); loss = 0.5808453559875488\n","Batch 134: 20/32 correct (62.5%); loss = 0.9180307388305664\n","Batch 135: 23/32 correct (71.9%); loss = 0.6544126868247986\n","Batch 136: 20/32 correct (62.5%); loss = 0.804264485836029\n","Batch 137: 26/32 correct (81.2%); loss = 0.5269594192504883\n","Batch 138: 26/32 correct (81.2%); loss = 0.659802258014679\n","Batch 139: 23/32 correct (71.9%); loss = 0.7057466506958008\n","Batch 140: 20/32 correct (62.5%); loss = 0.8207697868347168\n","Batch 141: 19/32 correct (59.4%); loss = 0.9005177021026611\n","Batch 142: 25/32 correct (78.1%); loss = 0.6483201384544373\n","Batch 143: 26/32 correct (81.2%); loss = 0.6374503970146179\n","Batch 144: 23/32 correct (71.9%); loss = 0.5823391079902649\n","Batch 145: 24/32 correct (75.0%); loss = 0.7242406010627747\n","Batch 146: 24/32 correct (75.0%); loss = 0.5133135318756104\n","Batch 147: 23/32 correct (71.9%); loss = 0.7027486562728882\n","Batch 148: 25/32 correct (78.1%); loss = 0.5642133355140686\n","Batch 149: 21/32 correct (65.6%); loss = 0.7706199884414673\n","Batch 150: 24/32 correct (75.0%); loss = 0.6789102554321289\n","Batch 151: 30/32 correct (93.8%); loss = 0.38311946392059326\n","Batch 152: 24/32 correct (75.0%); loss = 0.5941533446311951\n","Batch 153: 27/32 correct (84.4%); loss = 0.5620598793029785\n","Batch 154: 28/32 correct (87.5%); loss = 0.3780757784843445\n","Batch 155: 25/32 correct (78.1%); loss = 0.6098242402076721\n","Batch 156: 23/32 correct (71.9%); loss = 0.5699394941329956\n","Batch 157: 25/32 correct (78.1%); loss = 0.6711781024932861\n","Batch 158: 27/32 correct (84.4%); loss = 0.43111082911491394\n","Batch 159: 24/32 correct (75.0%); loss = 0.5820247530937195\n","Batch 160: 24/32 correct (75.0%); loss = 0.549910306930542\n","Batch 161: 24/32 correct (75.0%); loss = 0.5294233560562134\n","Batch 162: 24/32 correct (75.0%); loss = 0.647982656955719\n","Batch 163: 22/32 correct (68.8%); loss = 0.7820984125137329\n","Batch 164: 24/32 correct (75.0%); loss = 0.6817690134048462\n","Batch 165: 26/32 correct (81.2%); loss = 0.4744829535484314\n","Batch 166: 25/32 correct (78.1%); loss = 0.6762535572052002\n","Batch 167: 24/32 correct (75.0%); loss = 0.7245275974273682\n","Batch 168: 19/32 correct (59.4%); loss = 0.9245050549507141\n","Batch 169: 22/32 correct (68.8%); loss = 0.6920136213302612\n","Batch 170: 28/32 correct (87.5%); loss = 0.4517643451690674\n","Batch 171: 27/32 correct (84.4%); loss = 0.4820866584777832\n","Batch 172: 25/32 correct (78.1%); loss = 0.538265585899353\n","Batch 173: 22/32 correct (68.8%); loss = 0.8162047863006592\n","Batch 174: 25/32 correct (78.1%); loss = 0.46212518215179443\n","Batch 175: 20/32 correct (62.5%); loss = 0.880042552947998\n","Batch 176: 23/32 correct (71.9%); loss = 0.7400308847427368\n","Batch 177: 20/32 correct (62.5%); loss = 0.7848814129829407\n","Batch 178: 26/32 correct (81.2%); loss = 0.4276306629180908\n","Batch 179: 24/32 correct (75.0%); loss = 0.47639405727386475\n","Batch 180: 21/32 correct (65.6%); loss = 0.7827447056770325\n","Batch 181: 20/32 correct (62.5%); loss = 0.7132711410522461\n","Batch 182: 20/32 correct (62.5%); loss = 0.8468273282051086\n","Batch 183: 21/32 correct (65.6%); loss = 0.7894918918609619\n","Batch 184: 24/32 correct (75.0%); loss = 0.6290966272354126\n","Batch 185: 26/32 correct (81.2%); loss = 0.6118723154067993\n","Batch 186: 23/32 correct (71.9%); loss = 0.6651647090911865\n","Batch 187: 19/32 correct (59.4%); loss = 0.7179799675941467\n","Batch 188: 22/32 correct (68.8%); loss = 0.7271953821182251\n","Batch 189: 22/32 correct (68.8%); loss = 0.6063066720962524\n","Batch 190: 23/32 correct (71.9%); loss = 0.5940660238265991\n","Batch 191: 22/32 correct (68.8%); loss = 0.5644173622131348\n","Batch 192: 21/32 correct (65.6%); loss = 0.655574381351471\n","Batch 193: 26/32 correct (81.2%); loss = 0.5689020156860352\n","Batch 194: 25/32 correct (78.1%); loss = 0.6038890480995178\n","Batch 195: 21/32 correct (65.6%); loss = 0.7922237515449524\n","Batch 196: 28/32 correct (87.5%); loss = 0.34400641918182373\n","Batch 197: 27/32 correct (84.4%); loss = 0.5912758708000183\n","Batch 198: 21/32 correct (65.6%); loss = 0.762642502784729\n","Batch 199: 26/32 correct (81.2%); loss = 0.3751629590988159\n","Batch 200: 20/32 correct (62.5%); loss = 0.8086225986480713\n","Batch 201: 25/32 correct (78.1%); loss = 0.5620995163917542\n","Batch 202: 21/32 correct (65.6%); loss = 0.8245031833648682\n","Batch 203: 25/32 correct (78.1%); loss = 0.5069481730461121\n","Batch 204: 21/32 correct (65.6%); loss = 0.8849696516990662\n","Batch 205: 24/32 correct (75.0%); loss = 0.6975237131118774\n","Batch 206: 23/32 correct (71.9%); loss = 0.6141006350517273\n","Batch 207: 21/32 correct (65.6%); loss = 0.7740132808685303\n","Batch 208: 24/32 correct (75.0%); loss = 0.7090476155281067\n","Batch 209: 20/32 correct (62.5%); loss = 0.6339672803878784\n","Batch 210: 27/32 correct (84.4%); loss = 0.6614522337913513\n","Batch 211: 22/32 correct (68.8%); loss = 0.7905097007751465\n","Batch 212: 22/32 correct (68.8%); loss = 0.6904540657997131\n","Batch 213: 21/32 correct (65.6%); loss = 0.7799566388130188\n","Batch 214: 24/32 correct (75.0%); loss = 0.6801857948303223\n","Batch 215: 21/32 correct (65.6%); loss = 0.6497334241867065\n","Batch 216: 19/32 correct (59.4%); loss = 0.7927559614181519\n","Batch 217: 26/32 correct (81.2%); loss = 0.6585647463798523\n","Batch 218: 23/32 correct (71.9%); loss = 0.7126337289810181\n","Batch 219: 24/32 correct (75.0%); loss = 0.5975823402404785\n","Batch 220: 23/32 correct (71.9%); loss = 0.7183428406715393\n","Batch 221: 22/32 correct (68.8%); loss = 0.5999212861061096\n","Batch 222: 24/32 correct (75.0%); loss = 0.6742307543754578\n","Batch 223: 22/32 correct (68.8%); loss = 0.7951924204826355\n","Batch 224: 24/32 correct (75.0%); loss = 0.6878520250320435\n","Batch 225: 20/32 correct (62.5%); loss = 0.8743378520011902\n","Batch 226: 25/32 correct (78.1%); loss = 0.5804696679115295\n","Batch 227: 25/32 correct (78.1%); loss = 0.6353777050971985\n","Batch 228: 25/32 correct (78.1%); loss = 0.5096681118011475\n","Batch 229: 22/32 correct (68.8%); loss = 0.6233689785003662\n","Batch 230: 21/32 correct (65.6%); loss = 0.7386743426322937\n","Batch 231: 22/32 correct (68.8%); loss = 0.7017278671264648\n","Batch 232: 22/32 correct (68.8%); loss = 0.6444003582000732\n","Batch 233: 26/32 correct (81.2%); loss = 0.5043061971664429\n","Batch 234: 22/32 correct (68.8%); loss = 0.6401519775390625\n","Batch 235: 25/32 correct (78.1%); loss = 0.619261622428894\n","Batch 236: 23/32 correct (71.9%); loss = 0.7535811066627502\n","Batch 237: 27/32 correct (84.4%); loss = 0.5695372223854065\n","Batch 238: 25/32 correct (78.1%); loss = 0.4980655908584595\n","Batch 239: 24/32 correct (75.0%); loss = 0.6166388988494873\n","Batch 240: 22/32 correct (68.8%); loss = 0.7539557218551636\n","Batch 241: 25/32 correct (78.1%); loss = 0.5599600672721863\n","Batch 242: 25/32 correct (78.1%); loss = 0.5279951095581055\n","Batch 243: 24/32 correct (75.0%); loss = 0.5544936656951904\n","Batch 244: 23/32 correct (71.9%); loss = 0.705536425113678\n","Batch 245: 20/32 correct (62.5%); loss = 0.7886089086532593\n","Batch 246: 25/32 correct (78.1%); loss = 0.7074243426322937\n","Batch 247: 21/32 correct (65.6%); loss = 0.626212477684021\n","Batch 248: 24/32 correct (75.0%); loss = 0.47935137152671814\n","Batch 249: 24/32 correct (75.0%); loss = 0.543933093547821\n","Batch 250: 25/32 correct (78.1%); loss = 0.7090755105018616\n","Batch 251: 21/32 correct (65.6%); loss = 0.7488382458686829\n","Batch 252: 24/32 correct (75.0%); loss = 0.5772473812103271\n","Batch 253: 23/32 correct (71.9%); loss = 0.6203888654708862\n","Batch 254: 26/32 correct (81.2%); loss = 0.48277801275253296\n","Batch 255: 23/32 correct (71.9%); loss = 0.6399549841880798\n","Batch 256: 23/32 correct (71.9%); loss = 0.7172436714172363\n","Batch 257: 24/32 correct (75.0%); loss = 0.5974171161651611\n","Batch 258: 21/32 correct (65.6%); loss = 0.7205288410186768\n","Batch 259: 26/32 correct (81.2%); loss = 0.5533826947212219\n","Batch 260: 25/32 correct (78.1%); loss = 0.5494326949119568\n","Batch 261: 18/32 correct (56.2%); loss = 0.825636088848114\n","Batch 262: 24/32 correct (75.0%); loss = 0.6028817296028137\n","Batch 263: 25/32 correct (78.1%); loss = 0.6775261163711548\n","Batch 264: 23/32 correct (71.9%); loss = 0.6861938238143921\n","Batch 265: 23/32 correct (71.9%); loss = 0.6254646182060242\n","Batch 266: 27/32 correct (84.4%); loss = 0.49047619104385376\n","Batch 267: 26/32 correct (81.2%); loss = 0.45127224922180176\n","Epoch 1: 790/1101 correct (71.8%)\n","Epoch 2\n","Batch 1: 24/32 correct (75.0%); loss = 0.5912745594978333\n","Batch 2: 27/32 correct (84.4%); loss = 0.38615837693214417\n","Batch 3: 26/32 correct (81.2%); loss = 0.4290870726108551\n","Batch 4: 24/32 correct (75.0%); loss = 0.5789951086044312\n","Batch 5: 24/32 correct (75.0%); loss = 0.5499895215034485\n","Batch 6: 24/32 correct (75.0%); loss = 0.5184690952301025\n","Batch 7: 27/32 correct (84.4%); loss = 0.41132286190986633\n","Batch 8: 23/32 correct (71.9%); loss = 0.522911548614502\n","Batch 9: 26/32 correct (81.2%); loss = 0.5741597414016724\n","Batch 10: 28/32 correct (87.5%); loss = 0.27259281277656555\n","Batch 11: 24/32 correct (75.0%); loss = 0.4706587493419647\n","Batch 12: 30/32 correct (93.8%); loss = 0.39076605439186096\n","Batch 13: 28/32 correct (87.5%); loss = 0.3969098627567291\n","Batch 14: 25/32 correct (78.1%); loss = 0.5756499171257019\n","Batch 15: 25/32 correct (78.1%); loss = 0.39193475246429443\n","Batch 16: 29/32 correct (90.6%); loss = 0.29297998547554016\n","Batch 17: 24/32 correct (75.0%); loss = 0.6533718109130859\n","Batch 18: 26/32 correct (81.2%); loss = 0.47528037428855896\n","Batch 19: 27/32 correct (84.4%); loss = 0.4322205185890198\n","Batch 20: 25/32 correct (78.1%); loss = 0.4342384934425354\n","Batch 21: 23/32 correct (71.9%); loss = 0.4676821231842041\n","Batch 22: 25/32 correct (78.1%); loss = 0.5465717315673828\n","Batch 23: 23/32 correct (71.9%); loss = 0.5493842959403992\n","Batch 24: 26/32 correct (81.2%); loss = 0.4147486090660095\n","Batch 25: 24/32 correct (75.0%); loss = 0.5792409181594849\n","Batch 26: 27/32 correct (84.4%); loss = 0.41285884380340576\n","Batch 27: 24/32 correct (75.0%); loss = 0.3815780580043793\n","Batch 28: 24/32 correct (75.0%); loss = 0.6585760116577148\n","Batch 29: 22/32 correct (68.8%); loss = 0.5861139297485352\n","Batch 30: 26/32 correct (81.2%); loss = 0.4481319785118103\n","Batch 31: 27/32 correct (84.4%); loss = 0.45401111245155334\n","Batch 32: 24/32 correct (75.0%); loss = 0.6089152693748474\n","Batch 33: 29/32 correct (90.6%); loss = 0.28835567831993103\n","Batch 34: 25/32 correct (78.1%); loss = 0.48343706130981445\n","Batch 35: 25/32 correct (78.1%); loss = 0.49959689378738403\n","Batch 36: 27/32 correct (84.4%); loss = 0.5064868330955505\n","Batch 37: 28/32 correct (87.5%); loss = 0.27863866090774536\n","Batch 38: 26/32 correct (81.2%); loss = 0.4500257968902588\n","Batch 39: 21/32 correct (65.6%); loss = 0.6928354501724243\n","Batch 40: 26/32 correct (81.2%); loss = 0.5847188830375671\n","Batch 41: 24/32 correct (75.0%); loss = 0.43116721510887146\n","Batch 42: 26/32 correct (81.2%); loss = 0.44831058382987976\n","Batch 43: 25/32 correct (78.1%); loss = 0.5224736332893372\n","Batch 44: 19/32 correct (59.4%); loss = 0.8324001431465149\n","Batch 45: 27/32 correct (84.4%); loss = 0.5007991790771484\n","Batch 46: 27/32 correct (84.4%); loss = 0.40182286500930786\n","Batch 47: 27/32 correct (84.4%); loss = 0.45718348026275635\n","Batch 48: 26/32 correct (81.2%); loss = 0.5038385987281799\n","Batch 49: 28/32 correct (87.5%); loss = 0.36950331926345825\n","Batch 50: 25/32 correct (78.1%); loss = 0.5542404651641846\n","Batch 51: 24/32 correct (75.0%); loss = 0.6544540524482727\n","Batch 52: 25/32 correct (78.1%); loss = 0.7940949201583862\n","Batch 53: 28/32 correct (87.5%); loss = 0.4055999219417572\n","Batch 54: 23/32 correct (71.9%); loss = 0.5242878794670105\n","Batch 55: 23/32 correct (71.9%); loss = 0.5290407538414001\n","Batch 56: 28/32 correct (87.5%); loss = 0.43480080366134644\n","Batch 57: 25/32 correct (78.1%); loss = 0.4538174569606781\n","Batch 58: 27/32 correct (84.4%); loss = 0.40295374393463135\n","Batch 59: 30/32 correct (93.8%); loss = 0.3779270052909851\n","Batch 60: 27/32 correct (84.4%); loss = 0.43768465518951416\n","Batch 61: 26/32 correct (81.2%); loss = 0.3856537938117981\n","Batch 62: 26/32 correct (81.2%); loss = 0.47703269124031067\n","Batch 63: 21/32 correct (65.6%); loss = 0.7402389645576477\n","Batch 64: 26/32 correct (81.2%); loss = 0.4962575137615204\n","Batch 65: 25/32 correct (78.1%); loss = 0.45294660329818726\n","Batch 66: 24/32 correct (75.0%); loss = 0.557644784450531\n","Batch 67: 26/32 correct (81.2%); loss = 0.4336991012096405\n","Batch 68: 22/32 correct (68.8%); loss = 0.7299326062202454\n","Batch 69: 25/32 correct (78.1%); loss = 0.4878116846084595\n","Batch 70: 24/32 correct (75.0%); loss = 0.5547414422035217\n","Batch 71: 25/32 correct (78.1%); loss = 0.6202015280723572\n","Batch 72: 30/32 correct (93.8%); loss = 0.1945568323135376\n","Batch 73: 24/32 correct (75.0%); loss = 0.5387306213378906\n","Batch 74: 26/32 correct (81.2%); loss = 0.3695937991142273\n","Batch 75: 23/32 correct (71.9%); loss = 0.7416987419128418\n","Batch 76: 26/32 correct (81.2%); loss = 0.4599078893661499\n","Batch 77: 26/32 correct (81.2%); loss = 0.4606093466281891\n","Batch 78: 29/32 correct (90.6%); loss = 0.27193501591682434\n","Batch 79: 31/32 correct (96.9%); loss = 0.2781725525856018\n","Batch 80: 24/32 correct (75.0%); loss = 0.39747604727745056\n","Batch 81: 28/32 correct (87.5%); loss = 0.338082879781723\n","Batch 82: 20/32 correct (62.5%); loss = 0.5746277570724487\n","Batch 83: 27/32 correct (84.4%); loss = 0.6215433478355408\n","Batch 84: 27/32 correct (84.4%); loss = 0.43635356426239014\n","Batch 85: 29/32 correct (90.6%); loss = 0.291044145822525\n","Batch 86: 25/32 correct (78.1%); loss = 0.4502200484275818\n","Batch 87: 27/32 correct (84.4%); loss = 0.44753703474998474\n","Batch 88: 24/32 correct (75.0%); loss = 0.44358474016189575\n","Batch 89: 28/32 correct (87.5%); loss = 0.3095085322856903\n","Batch 90: 26/32 correct (81.2%); loss = 0.4358598589897156\n","Batch 91: 25/32 correct (78.1%); loss = 0.556972324848175\n","Batch 92: 30/32 correct (93.8%); loss = 0.3529178500175476\n","Batch 93: 25/32 correct (78.1%); loss = 0.6270931959152222\n","Batch 94: 29/32 correct (90.6%); loss = 0.37009698152542114\n","Batch 95: 27/32 correct (84.4%); loss = 0.5728129148483276\n","Batch 96: 25/32 correct (78.1%); loss = 0.5986076593399048\n","Batch 97: 27/32 correct (84.4%); loss = 0.4479159414768219\n","Batch 98: 28/32 correct (87.5%); loss = 0.3638073801994324\n","Batch 99: 25/32 correct (78.1%); loss = 0.4284881353378296\n","Batch 100: 24/32 correct (75.0%); loss = 0.8001207113265991\n","Batch 101: 26/32 correct (81.2%); loss = 0.42369571328163147\n","Batch 102: 28/32 correct (87.5%); loss = 0.4067222476005554\n","Batch 103: 28/32 correct (87.5%); loss = 0.40689244866371155\n","Batch 104: 24/32 correct (75.0%); loss = 0.5747144818305969\n","Batch 105: 26/32 correct (81.2%); loss = 0.4554446339607239\n","Batch 106: 28/32 correct (87.5%); loss = 0.39992213249206543\n","Batch 107: 25/32 correct (78.1%); loss = 0.6408000588417053\n","Batch 108: 28/32 correct (87.5%); loss = 0.31595146656036377\n","Batch 109: 22/32 correct (68.8%); loss = 0.6442975401878357\n","Batch 110: 26/32 correct (81.2%); loss = 0.39966562390327454\n","Batch 111: 29/32 correct (90.6%); loss = 0.3109179139137268\n","Batch 112: 25/32 correct (78.1%); loss = 0.46764805912971497\n","Batch 113: 26/32 correct (81.2%); loss = 0.6438628435134888\n","Batch 114: 26/32 correct (81.2%); loss = 0.5021566152572632\n","Batch 115: 27/32 correct (84.4%); loss = 0.37312430143356323\n","Batch 116: 25/32 correct (78.1%); loss = 0.662599503993988\n","Batch 117: 26/32 correct (81.2%); loss = 0.4268271028995514\n","Batch 118: 24/32 correct (75.0%); loss = 0.5581488013267517\n","Batch 119: 24/32 correct (75.0%); loss = 0.5913777947425842\n","Batch 120: 25/32 correct (78.1%); loss = 0.5651038885116577\n","Batch 121: 28/32 correct (87.5%); loss = 0.37833601236343384\n","Batch 122: 26/32 correct (81.2%); loss = 0.4806472957134247\n","Batch 123: 28/32 correct (87.5%); loss = 0.39021870493888855\n","Batch 124: 22/32 correct (68.8%); loss = 0.7236712574958801\n","Batch 125: 29/32 correct (90.6%); loss = 0.36603638529777527\n","Batch 126: 29/32 correct (90.6%); loss = 0.29182055592536926\n","Batch 127: 25/32 correct (78.1%); loss = 0.5022538304328918\n","Batch 128: 23/32 correct (71.9%); loss = 0.4971851408481598\n","Batch 129: 24/32 correct (75.0%); loss = 0.6393172144889832\n","Batch 130: 23/32 correct (71.9%); loss = 0.7638078331947327\n","Batch 131: 22/32 correct (68.8%); loss = 0.7018858194351196\n","Batch 132: 28/32 correct (87.5%); loss = 0.47362953424453735\n","Batch 133: 25/32 correct (78.1%); loss = 0.5947151184082031\n","Batch 134: 26/32 correct (81.2%); loss = 0.5601483583450317\n","Batch 135: 25/32 correct (78.1%); loss = 0.5373105406761169\n","Batch 136: 24/32 correct (75.0%); loss = 0.5350506901741028\n","Batch 137: 29/32 correct (90.6%); loss = 0.3310532867908478\n","Batch 138: 24/32 correct (75.0%); loss = 0.4740351736545563\n","Batch 139: 27/32 correct (84.4%); loss = 0.4814264476299286\n","Batch 140: 26/32 correct (81.2%); loss = 0.409145325422287\n","Batch 141: 25/32 correct (78.1%); loss = 0.4807092845439911\n","Batch 142: 26/32 correct (81.2%); loss = 0.6152281761169434\n","Batch 143: 22/32 correct (68.8%); loss = 0.7554478049278259\n","Batch 144: 27/32 correct (84.4%); loss = 0.35744622349739075\n","Batch 145: 28/32 correct (87.5%); loss = 0.44331270456314087\n","Batch 146: 25/32 correct (78.1%); loss = 0.5207975506782532\n","Batch 147: 29/32 correct (90.6%); loss = 0.3398948907852173\n","Batch 148: 26/32 correct (81.2%); loss = 0.42757248878479004\n","Batch 149: 24/32 correct (75.0%); loss = 0.5795248746871948\n","Batch 150: 26/32 correct (81.2%); loss = 0.44779396057128906\n","Batch 151: 27/32 correct (84.4%); loss = 0.5527645945549011\n","Batch 152: 25/32 correct (78.1%); loss = 0.4991818070411682\n","Batch 153: 26/32 correct (81.2%); loss = 0.5828145742416382\n","Batch 154: 25/32 correct (78.1%); loss = 0.44177576899528503\n","Batch 155: 28/32 correct (87.5%); loss = 0.3560540974140167\n","Batch 156: 27/32 correct (84.4%); loss = 0.3419797420501709\n","Batch 157: 29/32 correct (90.6%); loss = 0.3655151426792145\n","Batch 158: 24/32 correct (75.0%); loss = 0.6130048036575317\n","Batch 159: 26/32 correct (81.2%); loss = 0.4377000629901886\n","Batch 160: 26/32 correct (81.2%); loss = 0.3946448266506195\n","Batch 161: 25/32 correct (78.1%); loss = 0.6199854016304016\n","Batch 162: 29/32 correct (90.6%); loss = 0.35635554790496826\n","Batch 163: 29/32 correct (90.6%); loss = 0.3380485773086548\n","Batch 164: 27/32 correct (84.4%); loss = 0.40215176343917847\n","Batch 165: 25/32 correct (78.1%); loss = 0.5531579256057739\n","Batch 166: 26/32 correct (81.2%); loss = 0.5108022689819336\n","Batch 167: 29/32 correct (90.6%); loss = 0.2907644212245941\n","Batch 168: 30/32 correct (93.8%); loss = 0.2955758273601532\n","Batch 169: 28/32 correct (87.5%); loss = 0.2852618396282196\n","Batch 170: 23/32 correct (71.9%); loss = 0.6640770435333252\n","Batch 171: 23/32 correct (71.9%); loss = 0.4673178493976593\n","Batch 172: 20/32 correct (62.5%); loss = 0.8355720043182373\n","Batch 173: 28/32 correct (87.5%); loss = 0.32089170813560486\n","Batch 174: 27/32 correct (84.4%); loss = 0.35921576619148254\n","Batch 175: 28/32 correct (87.5%); loss = 0.4014330208301544\n","Batch 176: 28/32 correct (87.5%); loss = 0.29255396127700806\n","Batch 177: 26/32 correct (81.2%); loss = 0.5453994274139404\n","Batch 178: 28/32 correct (87.5%); loss = 0.31240713596343994\n","Batch 179: 28/32 correct (87.5%); loss = 0.3128982484340668\n","Batch 180: 24/32 correct (75.0%); loss = 0.5632122159004211\n","Batch 181: 27/32 correct (84.4%); loss = 0.5500138401985168\n","Batch 182: 26/32 correct (81.2%); loss = 0.4587416648864746\n","Batch 183: 27/32 correct (84.4%); loss = 0.4826570749282837\n","Batch 184: 27/32 correct (84.4%); loss = 0.42315101623535156\n","Batch 185: 26/32 correct (81.2%); loss = 0.6121276021003723\n","Batch 186: 21/32 correct (65.6%); loss = 0.8058598041534424\n","Batch 187: 28/32 correct (87.5%); loss = 0.2847292423248291\n","Batch 188: 25/32 correct (78.1%); loss = 0.48285841941833496\n","Batch 189: 26/32 correct (81.2%); loss = 0.5708960890769958\n","Batch 190: 23/32 correct (71.9%); loss = 0.6446828842163086\n","Batch 191: 26/32 correct (81.2%); loss = 0.6118959188461304\n","Batch 192: 28/32 correct (87.5%); loss = 0.4093286693096161\n","Batch 193: 28/32 correct (87.5%); loss = 0.4021660387516022\n","Batch 194: 26/32 correct (81.2%); loss = 0.49470973014831543\n","Batch 195: 23/32 correct (71.9%); loss = 0.692730188369751\n","Batch 196: 26/32 correct (81.2%); loss = 0.6131367087364197\n","Batch 197: 27/32 correct (84.4%); loss = 0.3942537307739258\n","Batch 198: 19/32 correct (59.4%); loss = 0.6232490539550781\n","Batch 199: 27/32 correct (84.4%); loss = 0.4299275875091553\n","Batch 200: 23/32 correct (71.9%); loss = 0.7274332642555237\n","Batch 201: 26/32 correct (81.2%); loss = 0.5168026089668274\n","Batch 202: 28/32 correct (87.5%); loss = 0.42464664578437805\n","Batch 203: 27/32 correct (84.4%); loss = 0.4402127265930176\n","Batch 204: 23/32 correct (71.9%); loss = 0.4975324273109436\n","Batch 205: 28/32 correct (87.5%); loss = 0.33028870820999146\n","Batch 206: 25/32 correct (78.1%); loss = 0.49197840690612793\n","Batch 207: 24/32 correct (75.0%); loss = 0.4486536681652069\n","Batch 208: 27/32 correct (84.4%); loss = 0.36173903942108154\n","Batch 209: 27/32 correct (84.4%); loss = 0.47903794050216675\n","Batch 210: 28/32 correct (87.5%); loss = 0.3708629310131073\n","Batch 211: 27/32 correct (84.4%); loss = 0.3462037742137909\n","Batch 212: 27/32 correct (84.4%); loss = 0.3074362874031067\n","Batch 213: 28/32 correct (87.5%); loss = 0.3156813979148865\n","Batch 214: 26/32 correct (81.2%); loss = 0.41227954626083374\n","Batch 215: 27/32 correct (84.4%); loss = 0.43549343943595886\n","Batch 216: 26/32 correct (81.2%); loss = 0.4747312366962433\n","Batch 217: 23/32 correct (71.9%); loss = 0.6550268530845642\n","Batch 218: 24/32 correct (75.0%); loss = 0.5163807272911072\n","Batch 219: 29/32 correct (90.6%); loss = 0.3637412488460541\n","Batch 220: 23/32 correct (71.9%); loss = 0.737492024898529\n","Batch 221: 26/32 correct (81.2%); loss = 0.5565346479415894\n","Batch 222: 23/32 correct (71.9%); loss = 0.6569731831550598\n","Batch 223: 24/32 correct (75.0%); loss = 0.6530618071556091\n","Batch 224: 26/32 correct (81.2%); loss = 0.46422573924064636\n","Batch 225: 24/32 correct (75.0%); loss = 0.5838536620140076\n","Batch 226: 25/32 correct (78.1%); loss = 0.41766953468322754\n","Batch 227: 25/32 correct (78.1%); loss = 0.5145646929740906\n","Batch 228: 22/32 correct (68.8%); loss = 0.5620058178901672\n","Batch 229: 23/32 correct (71.9%); loss = 0.5466154217720032\n","Batch 230: 27/32 correct (84.4%); loss = 0.4272869825363159\n","Batch 231: 25/32 correct (78.1%); loss = 0.5553197860717773\n","Batch 232: 23/32 correct (71.9%); loss = 0.6644681096076965\n","Batch 233: 27/32 correct (84.4%); loss = 0.3102652132511139\n","Batch 234: 26/32 correct (81.2%); loss = 0.526911199092865\n","Batch 235: 23/32 correct (71.9%); loss = 0.6391409039497375\n","Batch 236: 24/32 correct (75.0%); loss = 0.5655651688575745\n","Batch 237: 25/32 correct (78.1%); loss = 0.5521085858345032\n","Batch 238: 26/32 correct (81.2%); loss = 0.429056853055954\n","Batch 239: 26/32 correct (81.2%); loss = 0.5015795826911926\n","Batch 240: 27/32 correct (84.4%); loss = 0.46727797389030457\n","Batch 241: 25/32 correct (78.1%); loss = 0.4645957946777344\n","Batch 242: 23/32 correct (71.9%); loss = 0.5825275182723999\n","Batch 243: 23/32 correct (71.9%); loss = 0.5601101517677307\n","Batch 244: 29/32 correct (90.6%); loss = 0.28102824091911316\n","Batch 245: 29/32 correct (90.6%); loss = 0.4230062663555145\n","Batch 246: 23/32 correct (71.9%); loss = 0.570591926574707\n","Batch 247: 24/32 correct (75.0%); loss = 0.5333265662193298\n","Batch 248: 27/32 correct (84.4%); loss = 0.368633508682251\n","Batch 249: 26/32 correct (81.2%); loss = 0.49089694023132324\n","Batch 250: 27/32 correct (84.4%); loss = 0.3056473433971405\n","Batch 251: 23/32 correct (71.9%); loss = 0.7704575061798096\n","Batch 252: 27/32 correct (84.4%); loss = 0.359844446182251\n","Batch 253: 22/32 correct (68.8%); loss = 0.6658581495285034\n","Batch 254: 27/32 correct (84.4%); loss = 0.3863011598587036\n","Batch 255: 29/32 correct (90.6%); loss = 0.3927687108516693\n","Batch 256: 28/32 correct (87.5%); loss = 0.4237208068370819\n","Batch 257: 28/32 correct (87.5%); loss = 0.37617820501327515\n","Batch 258: 24/32 correct (75.0%); loss = 0.5373887419700623\n","Batch 259: 25/32 correct (78.1%); loss = 0.4852339029312134\n","Batch 260: 23/32 correct (71.9%); loss = 0.7047547101974487\n","Batch 261: 27/32 correct (84.4%); loss = 0.3570088744163513\n","Batch 262: 25/32 correct (78.1%); loss = 0.5889520645141602\n","Batch 263: 27/32 correct (84.4%); loss = 0.45064589381217957\n","Batch 264: 30/32 correct (93.8%); loss = 0.27454236149787903\n","Batch 265: 24/32 correct (75.0%); loss = 0.5251373052597046\n","Batch 266: 22/32 correct (68.8%); loss = 0.6862736940383911\n","Batch 267: 26/32 correct (81.2%); loss = 0.4264260530471802\n","Epoch 2: 822/1101 correct (74.7%)\n","Test: 1682/2210 correct (76.1%)\n"]}],"source":["\"\"\"\n","This script fine-tunes a BERT model on the SST.\n","\"\"\"\n","import torch\n","from torch import optim\n","from torchtext import data as tt\n","from torchtext.datasets import SST\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Load a pre-trained BERT model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      return_dict=True,\n","                                                      num_labels=4)\n","model.to(\"cuda\")\n","optimizer = optim.Adam(model.parameters(), lr=3e-5, eps=1e-8)\n","\n","# Load the data\n","text_field = tt.RawField()\n","label_field = tt.Field(sequential=False)\n","data = SST.splits(text_field, label_field)\n","label_field.build_vocab(data[0])\n","iters = tt.BucketIterator.splits(data, batch_size=32, device=\"cuda\")\n","\n","# Train\n","best_accuracy = 0\n","for epoch in range(2):\n","    print(\"Epoch\", epoch + 1)\n","    model.train()\n","    for i, batch in enumerate(iters[0]):\n","        model.zero_grad()\n","\n","        # Forward pass\n","        inputs = tokenizer(batch.text, return_tensors=\"pt\", padding=True)\n","        for k, v in inputs.items():\n","            if isinstance(v, torch.Tensor):\n","                inputs[k] = v.to(\"cuda\")\n","\n","        output = model(**inputs, labels=batch.label)\n","\n","        # Compute accuracy\n","        if True:\n","            predictions = output.logits.argmax(-1)\n","            num_correct = int(sum(predictions == batch.label))\n","            accuracy = num_correct / len(batch.label) * 100\n","            print(\"Batch {}: {}/{} correct ({:.1f}%); loss = {}\".format(\n","                i + 1, num_correct, len(batch.label), accuracy,\n","                float(output.loss)))\n","\n","        # Backward pass\n","        output.loss.backward()\n","        optimizer.step()\n","\n","    # Dev accuracy\n","    model.eval()\n","    model.zero_grad()\n","    num_correct = 0\n","    num_total = 0\n","    for batch in iters[1]:\n","        # Forward pass\n","        inputs = tokenizer(batch.text, return_tensors=\"pt\", padding=True)\n","        for k, v in inputs.items():\n","            if isinstance(v, torch.Tensor):\n","                inputs[k] = v.to(\"cuda\")\n","\n","        output = model(**inputs, labels=batch.label)\n","\n","        # Compute accuracy\n","        predictions = output.logits.argmax(-1)\n","        num_correct += int(sum(predictions == batch.label))\n","        num_total += len(batch.label)\n","\n","    accuracy = num_correct / num_total * 100\n","    print(\"Epoch {}: {}/{} correct ({:.1f}%)\".format(\n","        epoch + 1, num_correct, num_total, accuracy))\n","\n","    if accuracy > best_accuracy:\n","        torch.save(model.state_dict(), \"bert-sst.pt\")\n","        best_accuracy = accuracy\n","\n","# Testing\n","num_correct = 0\n","num_total = 0\n","for batch in iters[2]:\n","    # Forward pass\n","    inputs = tokenizer(batch.text, return_tensors=\"pt\", padding=True)\n","    for k, v in inputs.items():\n","        if isinstance(v, torch.Tensor):\n","            inputs[k] = v.to(\"cuda\")\n","\n","    output = model(**inputs, labels=batch.label)\n","\n","    # Compute accuracy\n","    predictions = output.logits.argmax(-1)\n","    num_correct += int(sum(predictions == batch.label))\n","    num_total += len(batch.label)\n","\n","accuracy = num_correct / num_total * 100\n","print(\"Test: {}/{} correct ({:.1f}%)\".format(num_correct, num_total, accuracy))\n","\n","torch.save(model.config, \"bert-sst-config.pt\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lTwWoUAS0cG","executionInfo":{"status":"ok","timestamp":1654782969346,"user_tz":-120,"elapsed":22468,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"86d6b67c-a981-4e25-f238-906793730379"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","def lrp_linear_single(x: np.ndarray, y: np.ndarray, rel_y: np.ndarray,\n","                      w: np.ndarray = None, eps: float = 0.001) -> np.ndarray:\n","    \"\"\"\n","    Implements the LRP-epsilon rule for a linear layer: y = w @ x + b.\n","    This function only computes LRP for a single example.\n","    \n","    :param x: Layer input (input_size,)\n","    :param y: Layer output (output_size,)\n","    :param rel_y: Network output relevance (output_size,)\n","    :param w: Weight matrix (input_size, output_size). If left blank,\n","        the weight matrix is assumed to be the identity: y = x + b\n","    :param eps: Stabilizer\n","    :return: The relevance of x (input_size,)\n","    \"\"\"\n","    y = y + eps * np.where(y >= 0, 1., -1.)\n","    if w is None:\n","        return x * (rel_y / y)\n","    return (w * x[:, np.newaxis]) @ (rel_y / y)\n","\n","\n","def lrp_linear(x: np.ndarray, y: np.ndarray, rel_y: np.ndarray,\n","               w: np.ndarray = None, eps: float = 0.001) -> np.ndarray:\n","    \"\"\"\n","    Implements the LRP-epsilon rule for a linear layer: y = w @ x + b.\n","    :param x: Input (..., input_size)\n","    :param y: Output (..., output_size)\n","    :param rel_y: Network output relevance (..., output_size)\n","    :param w: Transposed weight matrix (input_size, output_size). If\n","        left blank, the weight matrix is assumed to be the identity:\n","        y = x + b\n","    :param eps: Stabilizer\n","    :return: The relevance of x (batch_size, input_size)\n","    \"\"\"\n","    y = y + eps * np.where(y >= 0, 1., -1.)\n","    if w is None:\n","        return x * (rel_y / y)\n","\n","    lhs = w[..., np.newaxis, :, :] * x[..., np.newaxis]\n","    rhs = (rel_y / y)[..., np.newaxis]\n","    return (lhs @ rhs).squeeze(-1)\n","\n","\n","def lrp_matmul(x: np.ndarray, w: np.ndarray, y: np.ndarray, rel_y: np.ndarray,\n","               eps: float = 0.001) -> np.ndarray:\n","    \"\"\"\n","    LRP-epsilon for a matrix multiplication layer: y = w @ x. One of the\n","    two matrices is treated as a weight matrix. All matrices are assumed\n","    to be batched.\n","    :param x: Input (..., m, n)\n","    :param w: Weight matrix (..., p, m)\n","    :param y: Output (..., p, n)\n","    :param rel_y: Output relevance (..., p, n)\n","    :param eps: Stabilizer\n","    :return: The relevance of x (..., m, n)\n","    \"\"\"\n","    w = w.swapaxes(-1, -2)\n","    y = y + eps * np.where(y >= 0, 1., -1.)\n","\n","    lhs = np.moveaxis(w[..., np.newaxis] * x[..., np.newaxis, :], -1, -3)\n","    rhs = (rel_y / y).swapaxes(-1, -2)[..., np.newaxis]\n","    return (lhs @ rhs).squeeze(-1).swapaxes(-1, -2)"],"metadata":{"id":"qi9nhjIkYMdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from abc import ABC, abstractmethod\n","from typing import Tuple\n","\n","import numpy as np\n","import torch\n","from scipy.special import expit\n","from torch import nn\n","\n","\n","class BackpropModuleMixin(ABC):\n","    \"\"\"\n","    The general interface for modules with custom backward passes. When\n","    a module is in \"attribution mode,\" the forward and backward\n","    functions are replaced with the custom functions (re-)implemented in\n","    NumPy.\n","    \"\"\"\n","\n","    def __init__(self, *args, **kwargs):\n","        super(BackpropModuleMixin, self).__init__(*args, **kwargs)\n","        self.attr_mode = False  # Am I in attribution mode?\n","        self._input = None  # Forward pass input\n","        self._output = None  # Forward pass output\n","        self._state = None  # Forward pass stored computations\n","\n","    def train(self, *args, **kwargs):\n","        super(BackpropModuleMixin, self).train(*args, **kwargs)\n","        self.attr_mode = False\n","        self._input = None\n","        self._output = None\n","        self._state = None\n","\n","    def eval(self):\n","        super(BackpropModuleMixin, self).eval()\n","        self.attr_mode = False\n","        self._input = None\n","        self._output = None\n","        self._state = None\n","\n","    def attr(self):\n","        \"\"\"\n","        Puts the module in attribution mode.\n","        \"\"\"\n","        self.attr_mode = True\n","\n","    _convert_attr_input_to_numpy = True\n","\n","    def __call__(self, *args, **kwargs):\n","        if self.attr_mode:\n","            if self._convert_attr_input_to_numpy:\n","                args = list(args)\n","                for i in range(len(args)):\n","                    if isinstance(args[i], torch.Tensor):\n","                        args[i] = args[i].detach().numpy()\n","\n","                for k in kwargs:\n","                    if isinstance(kwargs[k], torch.Tensor):\n","                        kwargs[k] = kwargs[k].detach().numpy()\n","\n","            return self.attr_forward(*args, **kwargs)\n","        return super(BackpropModuleMixin, self).__call__(*args, **kwargs)\n","\n","    def backward(self, *args, **kwargs):\n","        if self.attr_mode:\n","            args = list(args)\n","            for i in range(len(args)):\n","                if isinstance(args[i], torch.Tensor):\n","                    args[i] = args[i].detach().numpy()\n","\n","            for k in kwargs:\n","                if isinstance(kwargs[k], torch.Tensor):\n","                    kwargs[k] = kwargs[k].detach().numpy()\n","\n","            return self.attr_backward(*args, **kwargs)\n","        self.backward(*args, **kwargs)\n","\n","    @abstractmethod\n","    def attr_forward(self, *args, **kwargs):\n","        \"\"\"\n","        The custom forward pass in NumPy.\n","        \"\"\"\n","        raise NotImplementedError(\"attr_forward not implemented\")\n","\n","    @abstractmethod\n","    def attr_backward(self, *args, **kwargs):\n","        \"\"\"\n","        The custom backward pass in NumPy.\n","        \"\"\"\n","        raise NotImplementedError(\"attr_backward not implemented\")\n","\n","\n","class BackpropLinear(BackpropModuleMixin, nn.Linear):\n","    \"\"\"\n","    An interface for nn.Linear.\n","    \"\"\"\n","\n","    def attr_forward(self, x: np.ndarray):\n","        self._input = [x]\n","        wx = x @ self.weight.detach().numpy().T\n","        self._state = dict(wx=wx)\n","        self._output = wx + self.bias.detach().numpy()\n","        return self._output\n","\n","\n","class BackpropRNNMixin(BackpropModuleMixin):\n","    \"\"\"\n","    An interface for PyTorch RNNs in general.\n","    \"\"\"\n","\n","    def __init__(self, *args, **kwargs):\n","        super(BackpropRNNMixin, self).__init__(*args, **kwargs,\n","                                               batch_first=True)\n","\n","    def attr_forward(self, x: np.ndarray):\n","        \"\"\"\n","        Computes the RNN forward pass for all layers and directions. The\n","        mathematical calculations are defined in the abstract helper\n","        function _layer_forward.\n","        :param x: An input to the RNN, of shape (batch_size, seq_len,\n","            input_size)\n","        :return: The RNN output, of shape (batch_size, seq_len,\n","            hidden_size)\n","        \"\"\"\n","        curr_input = x\n","        self._input = [None] * self.num_layers\n","        self._state = dict(ltr=[None] * self.num_layers)\n","        if self.bidirectional:\n","            self._state[\"rtl\"] = [None] * self.num_layers\n","\n","        for l in range(self.num_layers):\n","            self._layer_forward(curr_input, l, 0)\n","            if self.bidirectional:\n","                self._layer_forward(np.flip(curr_input, 1), l, 1)\n","                h_rev = np.flip(self._state[\"rtl\"][l][0], 1)\n","                curr_input = np.concatenate((self._state[\"ltr\"][l][0], h_rev),\n","                                            -1)\n","            else:\n","                curr_input = self._state[\"ltr\"][l][0]\n","\n","        self._output = curr_input\n","        return curr_input\n","\n","    @abstractmethod\n","    def _layer_forward(self, x: np.ndarray, layer: int, direction: int):\n","        \"\"\"\n","        This helper function computes the forward pass for a particular\n","        layer and direction.\n","        :param x: The input to the layer\n","        :param layer: The layer number\n","        :param direction: The direction number (0 for left to right, 1\n","            for right to left)\n","        :return: None, but the result should be stored in\n","            self._state[\"ltr\"][layer] or self._state[\"rtl\"][layer]\n","        \"\"\"\n","        raise NotImplementedError(\"_layer_forward not implemented\")\n","\n","    num_gates = None\n","\n","    def _params_numpy(self, prefix: str, layer: int, direction: int) \\\n","            -> Tuple[np.ndarray, ...]:\n","        \"\"\"\n","        Retrieves weight matrices or bias vectors for a particular layer\n","        and direction.\n","        :param prefix: \"weight_ih\" for input weights, \"weight_hh\" for\n","            hidden state weights, \"bias_ih\" for input biases, or\n","            \"bias_hh\" for hidden state biases\n","        :param layer: The layer to retrieve weights for\n","        :param direction: The direction to retrieve weights for\n","        :return: The weight/bias matrices\n","        \"\"\"\n","        p = prefix + \"_l\" + str(layer) + (\"_reverse\" if direction == 1 else \"\")\n","        return np.split(getattr(self, p).detach().numpy(), self.num_gates)\n","\n","\n","class BackpropLSTM(BackpropRNNMixin, nn.LSTM):\n","    \"\"\"\n","    An interface for nn.LSTM.\n","    \"\"\"\n","\n","    num_gates = 4\n","\n","    def _layer_forward(self, x: np.ndarray, layer: int, direction: int):\n","        if direction == 0:\n","            self._input[layer] = x\n","\n","        batch_size, seq_len, _ = x.shape\n","        x = x[:, :, :, np.newaxis]\n","\n","        # Get parameters\n","        kwargs = {\"layer\": layer, \"direction\": direction}\n","        w_ii, w_if, w_ig, w_io = self._params_numpy(\"weight_ih\", **kwargs)\n","        w_hi, w_hf, w_hg, w_ho = self._params_numpy(\"weight_hh\", **kwargs)\n","        biases_i = self._params_numpy(\"bias_ih\", **kwargs)\n","        biases_h = self._params_numpy(\"bias_hh\", **kwargs)\n","        b_ii, b_if, b_ig, b_io = [b[:, np.newaxis] for b in biases_i]\n","        b_hi, b_hf, b_hg, b_ho = [b[:, np.newaxis] for b in biases_h]\n","\n","        # Initialize\n","        h = np.zeros((batch_size, seq_len, self.hidden_size))\n","        i = np.zeros((batch_size, seq_len, self.hidden_size))\n","        f = np.zeros((batch_size, seq_len, self.hidden_size))\n","        g_pre = np.zeros((batch_size, seq_len, self.hidden_size))\n","        g = np.zeros((batch_size, seq_len, self.hidden_size))\n","        o = np.zeros((batch_size, seq_len, self.hidden_size))\n","        c = np.zeros((batch_size, seq_len, self.hidden_size))\n","\n","        # Forward pass\n","        h_prev = np.zeros((batch_size, self.hidden_size, 1))\n","        c_prev = np.zeros((batch_size, self.hidden_size))\n","        for t in range(seq_len):\n","            i_temp = (w_ii @ x[:, t] + b_ii + w_hi @ h_prev + b_hi).squeeze(-1)\n","            f_temp = (w_if @ x[:, t] + b_if + w_hf @ h_prev + b_hf).squeeze(-1)\n","            g_temp = (w_ig @ x[:, t] + b_ig + w_hg @ h_prev + b_hg).squeeze(-1)\n","            o_temp = (w_io @ x[:, t] + b_io + w_ho @ h_prev + b_ho).squeeze(-1)\n","\n","            i[:, t] = expit(i_temp)\n","            f[:, t] = expit(f_temp)\n","            g_pre[:, t] = g_temp\n","            g[:, t] = np.tanh(g_temp)\n","            o[:, t] = expit(o_temp)\n","\n","            c[:, t] = f[:, t] * c_prev + i[:, t] * g[:, t]\n","            h[:, t] = o[:, t] * np.tanh(c[:, t])\n","\n","            h_prev = h[:, t, :, np.newaxis]\n","            c_prev = c[:, t]\n","\n","        # Save trace to state\n","        if direction == 0:\n","            self._state[\"ltr\"][layer] = h, c, i, f, g, g_pre, w_ig.T, w_hg.T\n","        else:\n","            self._state[\"rtl\"][layer] = h, c, i, f, g, g_pre, w_ig.T, w_hg.T\n","\n","\n","class BackpropGRU(BackpropRNNMixin, nn.GRU):\n","    \"\"\"\n","    An interface for nn.GRU.\n","    \"\"\"\n","\n","    num_gates = 3\n","\n","    def _layer_forward(self, x: np.ndarray, layer: int, direction: int):\n","        if direction == 0:\n","            self._input[layer] = x\n","\n","        batch_size, seq_len, _ = x.shape\n","        x = x[:, :, :, np.newaxis]\n","\n","        # Get parameters\n","        kwargs = {\"layer\": layer, \"direction\": direction}\n","        w_ir, w_iz, w_in = self._params_numpy(\"weight_ih\", **kwargs)\n","        w_hr, w_hz, w_hn = self._params_numpy(\"weight_hh\", **kwargs)\n","        biases_i = self._params_numpy(\"bias_ih\", **kwargs)\n","        biases_h = self._params_numpy(\"bias_hh\", **kwargs)\n","        b_ir, b_iz, b_in = [b[:, np.newaxis] for b in biases_i]\n","        b_hr, b_hz, b_hn = [b[:, np.newaxis] for b in biases_h]\n","\n","        # Initialize\n","        h = np.zeros((batch_size, seq_len, self.hidden_size))\n","        r = np.zeros((batch_size, seq_len, self.hidden_size))\n","        z = np.zeros((batch_size, seq_len, self.hidden_size))\n","        n_pre = np.zeros((batch_size, seq_len, self.hidden_size))\n","        n = np.zeros((batch_size, seq_len, self.hidden_size))\n","\n","        # Forward pass\n","        h_prev = np.zeros((batch_size, self.hidden_size, 1))\n","        for t in range(seq_len):\n","            r_temp = (w_ir @ x[:, t] + b_ir + w_hr @ h_prev + b_hr).squeeze(-1)\n","            z_temp = (w_iz @ x[:, t] + b_iz + w_hz @ h_prev + b_hz).squeeze(-1)\n","            r[:, t] = expit(r_temp)\n","            z[:, t] = expit(z_temp)\n","\n","            n_pre_i = (w_in @ x[:, t] + b_in).squeeze(-1)\n","            n_pre_h = (w_hn @ h_prev + b_hn).squeeze(-1)\n","            n_pre[:, t] = n_pre_i + r[:, t] * n_pre_h\n","            n[:, t] = np.tanh(n_pre[:, t])\n","\n","            h[:, t] = (1 - z[:, t]) * n[:, t] + z[:, t] * h_prev.squeeze(-1)\n","            h_prev = h[:, t, :, np.newaxis]\n","\n","            # Save trace to state\n","            if direction == 0:\n","                self._state[\"ltr\"][layer] = h, r, z, n, n_pre, w_in.T, w_hn.T\n","            else:\n","                self._state[\"rtl\"][layer] = h, r, z, n, n_pre, w_in.T, w_hn.T\n","\n","\n","class BackpropLayerNorm(BackpropModuleMixin, nn.LayerNorm):\n","    \"\"\"\n","    Layer normalization for the Transformer.\n","    \"\"\"\n","\n","    def attr_forward(self, x):\n","        axes = tuple(range(-1, -len(self.normalized_shape) - 1, -1))\n","        mean = x.mean(axis=axes, keepdims=True)\n","        num = x - mean\n","        den = np.sqrt(x.var(axis=axes, keepdims=True) + self.eps)\n","\n","        self._state = dict(mean=mean, x=x)\n","        if not self.elementwise_affine:\n","            return num / den\n","\n","        gamma = self.weight.detach().numpy()\n","        beta = self.bias.detach().numpy()\n","        gamma_term = (num / den) * gamma\n","        output = gamma_term + beta\n","\n","        self._state[\"output\"] = output\n","        self._state[\"gamma_term\"] = gamma_term\n","        return output"],"metadata":{"id":"e9-2fKjBYTYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import List, Tuple, Union\n","\n","import numpy as np\n","import scipy.special as sp\n","import torch\n","from torch import nn\n","from transformers import BertConfig\n","from transformers.models.bert import modeling_bert as bert\n","\n","# from interpret_nlp.modules import backprop_module as bp\n","\n","# Shorthands for different array sizes\n","HiddenArray = np.ndarray  # (batch_size, seq_len, hidden_size)\n","AttentionArray = np.ndarray  # (batch_size, num_heads, seq_len, _)\n","IndexTensor = torch.LongTensor  # (batch_size, seq_len)\n","EmbeddingTensor = torch.FloatTensor  # (batch_size, seq_len, hidden_size)\n","\n","NormalLayer = Union[nn.Linear, nn.LayerNorm]\n","AttnSubLayer = Union[bert.BertSelfAttention, bert.BertSelfOutput]\n","\n","_erf_approx = lambda x: np.tanh(np.sqrt(2. / np.pi) * (x + 0.044715 * x ** 3))\n","activations = dict(relu=lambda x: np.maximum(x, 0.),\n","                   gelu=lambda x: x * .5 * (1. + sp.erf(x / np.sqrt(2.))),\n","                   swish=lambda x: x * sp.expit(x),\n","                   gelu_new=lambda x: x * .5 * _erf_approx(x),\n","                   mish=None)\n","\n","\n","def hidden_to_attention(h: HiddenArray, num_heads: int) -> AttentionArray:\n","    return h.reshape(h.shape[:-1] + (num_heads, -1)).transpose(0, 2, 1, 3)\n","\n","\n","def attention_to_hidden(a: AttentionArray) -> HiddenArray:\n","    a = a.transpose(0, 2, 1, 3)\n","    return a.reshape(a.shape[:-2] + (-1,))\n","\n","\n","class BackpropBertMixin(BackpropModuleMixin):\n","    \"\"\"\n","    Interface for BERT modules with custom backprop. This mixin\n","    introduces a function that converts a normal PyTorch module to a\n","    custom backprop module.\n","    \"\"\"\n","\n","    _layer_types = {nn.Linear: BackpropLinear,\n","                    nn.LayerNorm: BackpropLayerNorm}\n","    _bert_layer_types = {bert.BertSelfAttention: None,\n","                         bert.BertSelfOutput: None,\n","                         bert.BertAttention: None,\n","                         bert.BertIntermediate: None,\n","                         bert.BertOutput: None}\n","\n","    def convert_to_attr(self, layer: NormalLayer) -> BackpropModuleMixin:\n","        \"\"\"\n","        Converts nn.Linear or nn.LayerNorm to custom backprop layers. In\n","        order to use this, child classes must override the _layer_types\n","        dict defined above.\n","        :param layer: An nn.Linear or nn.LayerNorm module\n","        :return: The corresponding module with custom backprop\n","        \"\"\"\n","        if isinstance(layer, nn.Linear):\n","            linear_class = self._layer_types[nn.Linear]\n","            new_layer = linear_class(layer.in_features, layer.out_features)\n","        elif isinstance(layer, nn.LayerNorm):\n","            ln_class = self._layer_types[nn.LayerNorm]\n","            new_layer = ln_class(layer.normalized_shape, eps=layer.eps,\n","                                 elementwise_affine=layer.elementwise_affine)\n","        else:\n","            raise TypeError(\"Cannot convert layer of type \" + str(type(layer)))\n","\n","        new_layer.load_state_dict(layer.state_dict())\n","        return new_layer\n","\n","    def convert_bert_to_attr(self, layer: AttnSubLayer, config: BertConfig):\n","        new_layer = self._bert_layer_types[type(layer)](config)\n","        new_layer.load_state_dict(layer.state_dict())\n","        return new_layer\n","\n","    def hidden_to_attention(self, h: HiddenArray) -> AttentionArray:\n","        return hidden_to_attention(h, self.num_attention_heads)\n","\n","    @staticmethod\n","    def attention_to_hidden(a: AttentionArray) -> HiddenArray:\n","        return attention_to_hidden(a)\n","\n","\n","class BackpropBertEmbeddings(BackpropBertMixin, bert.BertEmbeddings):\n","    \"\"\"\n","    Combines word embeddings with positional embeddings and token type\n","    embeddings.\n","    \"\"\"\n","\n","    _convert_attr_input_to_numpy = False\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertEmbeddings, self).__init__(config)\n","        self.LayerNorm = self.convert_to_attr(self.LayerNorm)\n","\n","    def attr(self):\n","        super(BackpropBertEmbeddings, self).attr()\n","        self.LayerNorm.attr()\n","\n","    def attr_forward(self, input_ids: IndexTensor = None,\n","                     inputs_embeds: EmbeddingTensor = None,\n","                     token_type_ids: IndexTensor = None,\n","                     position_ids: IndexTensor = None) -> HiddenArray:\n","        \"\"\"\n","        Adds the word embeddings to positional and token type\n","        embeddings.\n","        :param input_ids: Indices for an input sequence\n","        :param inputs_embeds: Embeddings for an input sequence. Either\n","            this or input_ids must be none\n","        :param token_type_ids: idk what this is\n","        :param position_ids: Positions\n","        :return: The input to the first BERT layer\n","        \"\"\"\n","        assert (input_ids is None) != (inputs_embeds is None)\n","\n","        if input_ids is not None:\n","            input_shape = input_ids.shape\n","            inputs_embeds = self.word_embeddings(input_ids).detach().numpy()\n","        else:\n","            input_shape = inputs_embeds.shape[:-1]\n","            inputs_embeds = inputs_embeds.detach().numpy()\n","\n","        seq_length = input_shape[1]\n","        if position_ids is None:\n","            position_ids = self.position_ids[:, :seq_length]\n","        position_embeds = self.position_embeddings(position_ids)\n","        position_embeds = position_embeds.detach().numpy()\n","\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros(input_shape, dtype=torch.long,\n","                                         device=self.position_ids.device)\n","        token_type_embeds = self.token_type_embeddings(token_type_ids)\n","        token_type_embeds = token_type_embeds.detach().numpy()\n","\n","        self._state = inputs_embeds, position_embeds, token_type_embeds\n","        return self.LayerNorm(inputs_embeds + position_embeds +\n","                              token_type_embeds)\n","\n","\n","class BackpropBertSelfAttention(BackpropBertMixin, bert.BertSelfAttention):\n","    \"\"\"\n","    A BERT self-attention module. This module is responsible for\n","    implementing the scaled dot-product attention equation. This module\n","    is combined with BackpropBertSelfOutput to form an attention layer.\n","    \"\"\"\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertSelfAttention, self).__init__(config)\n","        self.query = self.convert_to_attr(self.query)\n","        self.key = self.convert_to_attr(self.key)\n","        self.value = self.convert_to_attr(self.value)\n","\n","    def attr(self):\n","        super(BackpropBertSelfAttention, self).attr()\n","        self.query.attr()\n","        self.key.attr()\n","        self.value.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     attention_mask: AttentionArray = None,\n","                     head_mask: AttentionArray = None,\n","                     encoder_hidden_states: HiddenArray = None,\n","                     encoder_attention_mask: AttentionArray = None) -> \\\n","            Tuple[HiddenArray, AttentionArray]:\n","        \"\"\"\n","        Implements the scaled dot-product attention equation.\n","        :param hidden_states: The input to the attention layer\n","        :param attention_mask: The attention mask\n","        :param head_mask: An optional mask for heads\n","        :param encoder_hidden_states: The encoder hidden states, passed\n","            to this layer when used in a decoder\n","        :param encoder_attention_mask: None\n","        :return: The result of the attention equation and the attention\n","            probabilities\n","        \"\"\"\n","        mixed_query_layer = self.query(hidden_states)\n","        if encoder_hidden_states is not None:\n","            mixed_key_layer = self.key(encoder_hidden_states)\n","            mixed_value_layer = self.value(encoder_hidden_states)\n","            attention_mask = encoder_attention_mask\n","        else:\n","            mixed_key_layer = self.key(hidden_states)\n","            mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.hidden_to_attention(mixed_query_layer)\n","        key_layer = self.hidden_to_attention(mixed_key_layer)\n","        value_layer = self.hidden_to_attention(mixed_value_layer)\n","\n","        attention_scores = query_layer @ key_layer.transpose(0, 1, 3, 2)\n","        attention_scores /= np.sqrt(self.attention_head_size)\n","        if attention_mask is not None:\n","            attention_scores += attention_mask\n","\n","        attention_probs = sp.softmax(attention_scores, axis=-1)\n","        if head_mask is not None:\n","            attention_probs *= head_mask\n","\n","        context_layer = attention_probs @ value_layer\n","        self._state = dict(context_layer=context_layer,\n","                           attention_probs=attention_probs,\n","                           value_layer=value_layer)\n","\n","        context_layer = self.attention_to_hidden(context_layer)\n","        return context_layer, attention_probs\n","\n","\n","class BackpropBertSelfOutput(BackpropBertMixin, bert.BertSelfOutput):\n","    \"\"\"\n","    Implements the attention heads and add-and-norm portion of a self-\n","    attention layer. This layer is used with BackpropBertSelfAttention.\n","    \"\"\"\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertSelfOutput, self).__init__(config)\n","        self.dense = self.convert_to_attr(self.dense)\n","        self.LayerNorm = self.convert_to_attr(self.LayerNorm)\n","\n","    def attr(self):\n","        super(BackpropBertSelfOutput, self).attr()\n","        self.dense.attr()\n","        self.LayerNorm.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     input_tensor: HiddenArray) -> HiddenArray:\n","        dense_output = self.dense(hidden_states)\n","        self._state = dict(dense_output=dense_output,\n","                           input_tensor=input_tensor)\n","        return self.LayerNorm(dense_output + input_tensor)\n","\n","\n","class BackpropBertAttention(BackpropBertMixin, bert.BertAttention):\n","    \"\"\"\n","    A complete self-attention layer, which combines\n","    BackpropBertSelfAttention with BackpropBertSelfOutput.\n","    \"\"\"\n","\n","    _bert_layer_types = {bert.BertSelfAttention: BackpropBertSelfAttention,\n","                         bert.BertSelfOutput: BackpropBertSelfOutput}\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertAttention, self).__init__(config)\n","        self.self = self.convert_bert_to_attr(self.self, config)\n","        self.output = self.convert_bert_to_attr(self.output, config)\n","\n","    def attr(self):\n","        super(BackpropBertAttention, self).attr()\n","        self.self.attr()\n","        self.output.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     attention_mask: AttentionArray = None,\n","                     head_mask: AttentionArray = None,\n","                     encoder_hidden_states: HiddenArray = None,\n","                     encoder_attention_mask: AttentionArray = None) -> \\\n","            Tuple[HiddenArray, AttentionArray]:\n","        \"\"\"\n","        :param hidden_states: The attention layer input (batch_size,\n","            seq_len, hidden_size)\n","        :param attention_mask: The attention mask\n","        :param head_mask:\n","        :param encoder_hidden_states:\n","        :param encoder_attention_mask:\n","        :return: The attention layer output\n","        \"\"\"\n","        self_outputs = self.self(hidden_states, attention_mask=attention_mask,\n","                                 head_mask=head_mask,\n","                                 encoder_hidden_states=encoder_hidden_states,\n","                                 encoder_attention_mask=encoder_attention_mask)\n","        return self.output(self_outputs[0], hidden_states), self_outputs[1]\n","\n","\n","class BackpropBertIntermediate(BackpropBertMixin, bert.BertIntermediate):\n","    \"\"\"\n","    Implements the first linear layer after the self-attention layer.\n","    \"\"\"\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertIntermediate, self).__init__(config)\n","        self.dense = self.convert_to_attr(self.dense)\n","        self.intermediate_act_fn_numpy = activations[config.hidden_act]\n","\n","    def attr(self):\n","        super(BackpropBertIntermediate, self).attr()\n","        self.dense.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray) -> HiddenArray:\n","        return self.intermediate_act_fn_numpy(self.dense(hidden_states))\n","\n","\n","class BackpropBertOutput(BackpropBertMixin, bert.BertOutput):\n","    \"\"\"\n","    Implements the final linear and add-and-norm layers of a Transformer\n","    encoder/decoder block.\n","    \"\"\"\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertOutput, self).__init__(config)\n","        self.dense = self.convert_to_attr(self.dense)\n","        self.LayerNorm = self.convert_to_attr(self.LayerNorm)\n","\n","    def attr(self):\n","        super(BackpropBertOutput, self).attr()\n","        self.dense.attr()\n","        self.LayerNorm.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     input_tensor: HiddenArray) -> HiddenArray:\n","        dense_output = self.dense(hidden_states)\n","        self._state = dict(dense_output=dense_output,\n","                           input_tensor=input_tensor)\n","        return self.LayerNorm(dense_output + input_tensor)\n","\n","\n","class BackpropBertLayer(BackpropBertMixin, bert.BertLayer):\n","    \"\"\"\n","    A full BERT encoder or decoder block.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertAttention: BackpropBertAttention,\n","                         bert.BertIntermediate: BackpropBertIntermediate,\n","                         bert.BertOutput: BackpropBertOutput}\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertLayer, self).__init__(config)\n","        self.attention = self.convert_bert_to_attr(self.attention, config)\n","        if self.add_cross_attention:\n","            self.crossattention = self.convert_bert_to_attr(\n","                self.crossattention, config)\n","        self.intermediate = self.convert_bert_to_attr(self.intermediate,\n","                                                      config)\n","        self.output = self.convert_bert_to_attr(self.output, config)\n","\n","    def attr(self):\n","        super(BackpropBertLayer, self).attr()\n","        self.attention.attr()\n","        if self.add_cross_attention:\n","            self.crossattention.attr()\n","        self.intermediate.attr()\n","        self.output.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     attention_mask: AttentionArray = None,\n","                     head_mask: AttentionArray = None,\n","                     encoder_hidden_states: HiddenArray = None,\n","                     encoder_attention_mask: AttentionArray = None) -> \\\n","            Tuple[HiddenArray, AttentionArray]:\n","        \"\"\"\n","        The complete forward pass for a full encoder or decoder block.\n","        :param hidden_states: The input to the encoder or decoder block\n","        :param attention_mask: The attention mask\n","        :param head_mask: The head mask\n","        :param encoder_hidden_states: Hidden states from the encoder, if\n","            this is a decoder block\n","        :param encoder_attention_mask: The encoder attention mask, if\n","            this is a decoder block\n","        :return: The output of this block, along with the attention\n","            scores\n","        \"\"\"\n","        assert self.attention.attr_mode\n","        assert self.intermediate.attr_mode\n","        assert self.output.attr_mode\n","        if self.add_cross_attention:\n","            assert self.crossattention.attr_mode\n","\n","        attn_output, attn_probs = self.attention(hidden_states,\n","                                                 attention_mask=attention_mask,\n","                                                 head_mask=head_mask)\n","\n","        if self.is_decoder and encoder_hidden_states is not None:\n","            self._state = {\"crossattention_used\": True}\n","            assert hasattr(self, \"crossattention\")\n","            assert self.crossattention.attr_mode\n","            cross_output = self.crossattention(attn_output,\n","                                               attention_mask, head_mask,\n","                                               encoder_hidden_states,\n","                                               encoder_attention_mask)\n","            attn_output, attn_probs = cross_output\n","        else:\n","            self._state = {\"crossattention_used\": False}\n","\n","        # TODO: Make apply_chunking_to_forward compatible with NumPy\n","        output = bert.apply_chunking_to_forward(self.feed_forward_chunk,\n","                                                self.chunk_size_feed_forward,\n","                                                self.seq_len_dim, attn_output)\n","\n","        return output, attn_probs\n","\n","\n","class BackpropBertEncoder(BackpropBertMixin, bert.BertEncoder):\n","    \"\"\"\n","    A BERT encoder, consisting of multiple encoder blocks.\n","    \"\"\"\n","\n","    _bert_layer_types = {bert.BertLayer: BackpropBertLayer}\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertEncoder, self).__init__(config)\n","        layers = [self.convert_bert_to_attr(e, config) for e in self.layer]\n","        self.layer = nn.ModuleList(layers)\n","\n","    def attr(self):\n","        super(BackpropBertEncoder, self).attr()\n","        for e in self.layer:\n","            e.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray,\n","                     attention_mask: AttentionArray = None,\n","                     head_mask: AttentionArray = None,\n","                     encoder_hidden_states: HiddenArray = None,\n","                     encoder_attention_mask: AttentionArray = None) -> \\\n","            Tuple[HiddenArray, List[HiddenArray], List[AttentionArray]]:\n","        \"\"\"\n","        A full BERT encoder, consisting of multiple encoder blocks.\n","        :param hidden_states: The combined word, position, and token\n","            type embeddings\n","        :param attention_mask: The attention mask\n","        :param head_mask: The head mask\n","        :param encoder_hidden_states: ???\n","        :param encoder_attention_mask: ???\n","        :return: The output of the last layer, along with the outputs\n","            and attention scores of all layers\n","        \"\"\"\n","        all_hidden_states = []\n","        all_attentions = []\n","\n","        for i, e in enumerate(self.layer):\n","            all_hidden_states.append(hidden_states)\n","\n","            # TODO: Add gradient checkpointing\n","            layer_outputs = e(hidden_states, attention_mask=attention_mask,\n","                              head_mask=head_mask[i],\n","                              encoder_hidden_states=encoder_hidden_states)\n","\n","            hidden_states = layer_outputs[0]\n","            all_attentions.append(layer_outputs[1])\n","\n","        return hidden_states, all_hidden_states, all_attentions\n","\n","\n","class BackpropBertPooler(BackpropBertMixin, bert.BertPooler):\n","    \"\"\"\n","    A layer that \"pools\" the BERT output by passing the CLS output\n","    through a tanh.\n","    \"\"\"\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertPooler, self).__init__(config)\n","        self.dense = self.convert_to_attr(self.dense)\n","\n","    def attr(self):\n","        super(BackpropBertPooler, self).attr()\n","        self.dense.attr()\n","\n","    def attr_forward(self, hidden_states: HiddenArray) -> np.ndarray:\n","        return np.tanh(self.dense(hidden_states[:, 0]))\n","\n","\n","class BackpropBertModel(BackpropBertMixin, bert.BertModel):\n","    \"\"\"\n","    A full BERT model. This is a stack of Transformer encoders that\n","    takes an input sequence of the form\n","        [CLS] sequence1 [SEP] sequence2\n","    and produces an output sequence of the same form. It is pre-trained\n","    on BERT's masked language modeling objective.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertEmbeddings: BackpropBertEmbeddings,\n","                         bert.BertEncoder: BackpropBertEncoder,\n","                         bert.BertPooler: BackpropBertPooler}\n","\n","    _convert_attr_input_to_numpy = False\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertModel, self).__init__(config)\n","        self.embeddings = self.convert_bert_to_attr(self.embeddings, config)\n","        self.encoder = self.convert_bert_to_attr(self.encoder, config)\n","        self.pooler = self.convert_bert_to_attr(self.pooler, config)\n","\n","    def attr(self):\n","        super(BackpropBertModel, self).attr()\n","        self.embeddings.attr()\n","        self.encoder.attr()\n","        self.pooler.attr()\n","\n","    def attr_forward(self, input_ids=None, attention_mask=None,\n","                     token_type_ids=None, position_ids=None, head_mask=None,\n","                     inputs_embeds=None, encoder_hidden_states=None,\n","                     encoder_attention_mask=None):\n","        \"\"\"\n","        The complete BERT forward pass.\n","        :param input_ids: An input sequence, represented as an index\n","            tensor of shape (batch_size, seq_len)\n","        :param attention_mask: An attention mask that masks out [PAD]\n","            symbols and symbols without a prediction\n","        :param token_type_ids: Not sure what this is for\n","        :param position_ids: The positional encoding\n","        :param head_mask: Some other mask\n","        :param inputs_embeds: Embedding vectors for the input. This\n","            cannot be specified if input_ids is specified, and vice\n","            versa\n","        :param encoder_hidden_states: Hidden states from a previous\n","            computation, which will be reused\n","        :param encoder_attention_mask: The attention mask from a\n","            previous computation, which will be reused\n","        :return: The sequence output, the pooled output, and all the\n","            encoder block outputs\n","        \"\"\"\n","        # Get input embedding shape\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and \"\n","                             \"inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = input_ids.shape\n","        elif inputs_embeds is not None:\n","            input_shape = inputs_embeds.shape[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or \"\n","                             \"inputs_embeds\")\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones(input_shape)\n","        if token_type_ids is None:\n","            token_type_ids = np.zeros(input_shape, dtype=\"int64\")\n","\n","        # Not really sure what this is for\n","        extended_attention_mask = \\\n","            self.get_extended_attention_mask(attention_mask, input_shape,\n","                                             torch.device(\"cpu\"))\n","        extended_attention_mask = extended_attention_mask.detach().numpy()\n","\n","        if self.config.is_decoder and encoder_hidden_states is not None:\n","            raise RuntimeWarning(\"I didn't implement this carefully\")\n","            encoder_hidden_shape = encoder_hidden_states.shape[:-1]\n","            if encoder_attention_mask is None:\n","                encoder_attention_mask = torch.ones(encoder_hidden_shape)\n","            encoder_extended_attn_mask = \\\n","                self.invert_attention_mask(encoder_attention_mask)\n","            encoder_extended_attn_mask = \\\n","                encoder_extended_attn_mask.detach().numpy()\n","        else:\n","            encoder_extended_attn_mask = None\n","\n","        head_mask = self.get_head_mask(head_mask,\n","                                       self.config.num_hidden_layers)\n","\n","        # Begin forward pass\n","        embedding_output = self.embeddings(input_ids=input_ids,\n","                                           position_ids=position_ids,\n","                                           token_type_ids=token_type_ids,\n","                                           inputs_embeds=inputs_embeds)\n","\n","        encoder_outputs = \\\n","            self.encoder(embedding_output,\n","                         attention_mask=extended_attention_mask,\n","                         head_mask=head_mask,\n","                         encoder_hidden_states=encoder_hidden_states,\n","                         encoder_attention_mask=encoder_extended_attn_mask)\n","\n","        sequence_output = encoder_outputs[0]\n","        self._state = {\"output_shape\": sequence_output.shape}\n","        pooled_output = self.pooler(sequence_output)\n","        return (sequence_output, pooled_output) + encoder_outputs[1:]\n","\n","\n","BFSC = bert.BertForSequenceClassification\n","\n","\n","class BackpropBertForSequenceClassification(BackpropBertMixin, BFSC):\n","    \"\"\"\n","    A BERT model with a linear decoder.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertModel: BackpropBertModel}\n","\n","    _convert_attr_input_to_numpy = False\n","\n","    def __init__(self, config: BertConfig):\n","        super(BackpropBertForSequenceClassification, self).__init__(config)\n","        self.bert = self.convert_bert_to_attr(self.bert, config)\n","        self.classifier = self.convert_to_attr(self.classifier)\n","\n","    def attr(self):\n","        super(BackpropBertForSequenceClassification, self).attr()\n","        self.bert.attr()\n","        self.classifier.attr()\n","\n","    def attr_forward(self, **kwargs):\n","        outputs = self.bert(**kwargs)\n","        return self.classifier(outputs[1])"],"metadata":{"id":"PmTSYuLGY05R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import operator\n","from abc import ABC, abstractmethod\n","from functools import reduce\n","\n","import numpy as np\n","\n","\n","\n","class LRPLinear(BackpropLinear):\n","    \"\"\"\n","    A Linear module with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: np.ndarray,\n","                      eps: float = 0.001) -> np.ndarray:\n","        return lrp_linear(self._input[0], self._state[\"wx\"], rel_y,\n","                          self.weight.detach().numpy().T, eps=eps)\n","\n","\n","class LRPRNNMixin(ABC):\n","    \"\"\"\n","    An interface for RNNs with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: np.ndarray,\n","                      eps: float = 0.001) -> np.ndarray:\n","        \"\"\"\n","        Computes the LRP backward pass using the helper function\n","        _layer_backward.\n","        :param rel_y: The relevance of the RNN output, of shape\n","            (batch_size, seq_len, hidden_size)\n","        :param eps: The LRP stabilizer\n","        :return: The relevance of the stored input\n","        \"\"\"\n","        if self.bidirectional:\n","            curr_rel, curr_rel_rev = np.split(rel_y, 2, axis=-1)\n","            curr_rel_rev = np.flip(curr_rel_rev, 1)\n","        else:\n","            curr_rel = rel_y\n","            curr_rel_rev = None\n","\n","        for l in reversed(range(self.num_layers)):\n","            rel_x = self._layer_backward(curr_rel, l, 0, eps=eps)\n","            if self.bidirectional:\n","                rel_x_rev = self._layer_backward(curr_rel_rev, l, 1, eps=eps)\n","                rel_x += np.flip(rel_x_rev, 1)\n","\n","            if self.bidirectional and l > 0:\n","                curr_rel = rel_x[:, :, :self.hidden_size]\n","                curr_rel_rev = np.flip(rel_x[:, :, self.hidden_size:], 1)\n","            else:\n","                curr_rel = rel_x\n","\n","        return curr_rel\n","\n","    @abstractmethod\n","    def _layer_backward(self, rel_y: np.ndarray, layer: int, direction: int,\n","                        eps: float = 0.001) -> np.ndarray:\n","        raise NotImplementedError(\"_layer_backward not implemented\")\n","\n","\n","class LRPLSTM(LRPRNNMixin, BackpropLSTM):\n","    \"\"\"\n","    An LSTM module with LRP.\n","    \"\"\"\n","\n","    def _layer_backward(self, rel_y: np.ndarray, layer: int, direction: int,\n","                        eps: float = 0.001) -> np.ndarray:\n","        \"\"\"\n","        Performs a backward pass using numpy operations for one layer.\n","        :param rel_y: The relevance flowing to this layer\n","        :param layer: The layer to perform the backward pass for\n","        :param direction: The direction to perform the backward pass for\n","        :return: The relevance of the layer inputs\n","        \"\"\"\n","        if direction == 0:\n","            x = self._input[layer]\n","            h, c, i, f, g, g_pre, w_ig, w_hg = self._state[\"ltr\"][layer]\n","        else:\n","            x = np.flip(self._input[layer], 1)\n","            h, c, i, f, g, g_pre, w_ig, w_hg = self._state[\"rtl\"][layer]\n","\n","        batch_size, seq_len, _ = x.shape\n","\n","        # Initialize\n","        rel_h = np.zeros((batch_size, seq_len + 1, self.hidden_size))\n","        rel_c = np.zeros((batch_size, seq_len + 1, self.hidden_size))\n","        rel_g = np.zeros(g.shape)\n","        rel_x = np.zeros(x.shape)\n","\n","        # Backward pass\n","        rel_h[:, 1:] = rel_y\n","        for t in reversed(range(seq_len)):\n","            rel_c[:, t + 1] += rel_h[:, t + 1]\n","            rel_c[:, t] = lrp_linear(f[:, t] * c[:, t - 1], c[:, t],\n","                                     rel_c[:, t + 1], eps=eps)\n","            rel_g[:, t] = lrp_linear(i[:, t] * g[:, t], c[:, t],\n","                                     rel_c[:, t + 1], eps=eps)\n","            rel_x[:, t] = lrp_linear(x[:, t], g_pre[:, t], rel_g[:, t],\n","                                     w=w_ig, eps=eps)\n","\n","            h_prev = np.zeros((batch_size, self.hidden_size)) if t == 0 \\\n","                else h[:, t - 1]\n","            rel_h[:, t] += lrp_linear(h_prev, g_pre[:, t], rel_g[:, t], w=w_hg,\n","                                      eps=eps)\n","\n","        return rel_x\n","\n","\n","class LRPGRU(LRPRNNMixin, BackpropGRU):\n","    \"\"\"\n","    A GRU module with LRP.\n","    \"\"\"\n","\n","    def _layer_backward(self, rel_y: np.ndarray, layer: int, direction: int,\n","                        eps: float = 0.001) -> np.ndarray:\n","        \"\"\"\n","        Performs a backward pass using numpy operations for one layer.\n","        :param rel_y: The relevance flowing to this layer\n","        :param layer: The layer to perform the backward pass for\n","        :param direction: The direction to perform the backward pass for\n","        :return: The relevance of the layer inputs\n","        \"\"\"\n","        if direction == 0:\n","            x = self._input[layer]\n","            h, r, z, n, n_pre, w_in, w_hn = self._state[\"ltr\"][layer]\n","        else:\n","            x = np.flip(self._input[layer], 1)\n","            h, r, z, n, n_pre, w_in, w_hn = self._state[\"rtl\"][layer]\n","\n","        batch_size, seq_len, _ = x.shape\n","\n","        # Initialize\n","        rel_h = np.zeros((batch_size, seq_len + 1, self.hidden_size))\n","        rel_n = np.zeros(n.shape)\n","        rel_x = np.zeros(x.shape)\n","\n","        # Backward pass\n","        rel_h[:, 1:] = rel_y\n","        for t in reversed(range(seq_len)):\n","            rel_h[:, t] = lrp_linear(z[:, t] * n[:, t], h[:, t],\n","                                     rel_h[:, t + 1], eps=eps)\n","            rel_n[:, t] = lrp_linear((1 - z[:, t]) * n[:, t], h[:, t],\n","                                     rel_h[:, t + 1], eps=eps)\n","            rel_x[:, t] = lrp_linear(x[:, t], n_pre[:, t], rel_n[:, t],\n","                                     w=w_in, eps=eps)\n","\n","            h_prev = np.zeros((batch_size, self.hidden_size)) if t == 0 \\\n","                else h[:, t - 1]\n","            rel_h[:, t] += lrp_linear(h_prev, n_pre[:, t], rel_n[:, t],\n","                                      w=r[:, t] * w_hn, eps=eps)\n","\n","        return rel_x\n","\n","\n","class LRPLayerNorm(BackpropLayerNorm):\n","    \"\"\"\n","    A LayerNorm module with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: np.ndarray, eps: float = 0.001) -> \\\n","            np.ndarray:\n","        \"\"\"\n","        :param rel_y:\n","        :param eps:\n","        :return:\n","        \"\"\"\n","        if self.elementwise_affine:\n","            rel_y = lrp_linear(self._state[\"gamma_term\"],\n","                               self._state[\"output\"], rel_y, eps=eps)\n","\n","        num = self._state[\"x\"] - self._state[\"mean\"]\n","        rel_x = lrp_linear(self._state[\"x\"], num, rel_y, eps=eps)\n","        rel_mean = lrp_linear(-self._state[\"mean\"], num, rel_y, eps=eps)\n","\n","        n = reduce(operator.mul, self.normalized_shape, 1)\n","        rel_x += lrp_linear(self._state[\"x\"], n * self._state[\"mean\"],\n","                            rel_mean, eps=eps)\n","\n","        return rel_x"],"metadata":{"id":"oFSytW1jZp1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Tuple\n","\n","import numpy as np\n","from torch import nn\n","from transformers.models.bert import modeling_bert as bert\n","\n","\n","\n","\n","class LRPBertMixin(BackpropBertMixin):\n","    _layer_types = {nn.Linear: LRPLinear,\n","                    nn.LayerNorm: LRPLayerNorm}\n","\n","\n","class LRPBertEmbeddings(LRPBertMixin, BackpropBertEmbeddings):\n","    \"\"\"\n","    BertEmbeddings with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            Tuple[HiddenArray, HiddenArray, HiddenArray]:\n","        \"\"\"\n","        :param rel_y:\n","        :param eps:\n","        :return:\n","        \"\"\"\n","        rel_y = self.LayerNorm.attr_backward(rel_y, eps=eps)\n","\n","        inp_embeds, pos_embeds, tok_type_embeds = self._state\n","        combined_embeds = inp_embeds + pos_embeds + tok_type_embeds\n","        rel_input = lrp_linear(inp_embeds, combined_embeds, rel_y, eps=eps)\n","        rel_pos = lrp_linear(pos_embeds, combined_embeds, rel_y, eps=eps)\n","        rel_tok = lrp_linear(tok_type_embeds, combined_embeds, rel_y, eps=eps)\n","        return rel_input, rel_pos, rel_tok\n","\n","\n","class LRPBertSelfAttention(LRPBertMixin, BackpropBertSelfAttention):\n","    \"\"\"\n","    BertSelfAttention with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        \"\"\"\n","        All relevance gets propagated to the value layer.\n","        :param rel_y:\n","        :param eps:\n","        :return:\n","        \"\"\"\n","        rel_value_layer = lrp_matmul(self._state[\"value_layer\"],\n","                                     self._state[\"attention_probs\"],\n","                                     self._state[\"context_layer\"],\n","                                     self.hidden_to_attention(rel_y),\n","                                     eps=eps)\n","\n","        rel_value_layer = self.attention_to_hidden(rel_value_layer)\n","        return self.value.attr_backward(rel_value_layer)\n","\n","\n","class LRPBertSelfOutput(LRPBertMixin, BackpropBertSelfOutput):\n","    \"\"\"\n","    BertSelfOutput with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            Tuple[HiddenArray, HiddenArray]:\n","        input_tensor = self._state[\"input_tensor\"]\n","        dense_output = self._state[\"dense_output\"]\n","        pre_layer_norm = input_tensor + dense_output\n","\n","        rel_pre_layer_norm = self.LayerNorm.attr_backward(rel_y)\n","        rel_input_tensor = lrp_linear(input_tensor, pre_layer_norm,\n","                                      rel_pre_layer_norm, eps=eps)\n","        rel_dense_output = lrp_linear(dense_output, pre_layer_norm,\n","                                      rel_pre_layer_norm, eps=eps)\n","        rel_hidden_states = self.dense.attr_backward(rel_dense_output)\n","\n","        return rel_hidden_states, rel_input_tensor\n","\n","\n","class LRPBertAttention(LRPBertMixin, BackpropBertAttention):\n","    \"\"\"\n","    BertAttention with LRP.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertSelfAttention: LRPBertSelfAttention,\n","                         bert.BertSelfOutput: LRPBertSelfOutput}\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        rel_hidden, rel_input = self.output.attr_backward(rel_y, eps=eps)\n","        rel_input += self.self.attr_backward(rel_hidden, eps=eps)\n","        return rel_input\n","\n","\n","class LRPBertIntermediate(LRPBertMixin, BackpropBertIntermediate):\n","    \"\"\"\n","    BertIntermediate with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        return self.dense.attr_backward(rel_y, eps=eps)\n","\n","\n","class LRPBertOutput(LRPBertMixin, BackpropBertOutput):\n","    \"\"\"\n","    BertOutput with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            Tuple[HiddenArray, HiddenArray]:\n","        input_tensor = self._state[\"input_tensor\"]\n","        dense_output = self._state[\"dense_output\"]\n","        pre_layer_norm = input_tensor + dense_output\n","\n","        rel_pre_layer_norm = self.LayerNorm.attr_backward(rel_y)\n","        rel_input_tensor = lrp_linear(input_tensor, pre_layer_norm,\n","                                      rel_pre_layer_norm, eps=eps)\n","        rel_dense_output = lrp_linear(dense_output, pre_layer_norm,\n","                                      rel_pre_layer_norm, eps=eps)\n","        rel_hidden_states = self.dense.attr_backward(rel_dense_output)\n","\n","        return rel_hidden_states, rel_input_tensor\n","\n","\n","class LRPBertLayer(BackpropBertLayer):\n","    \"\"\"\n","    BertLayer with LRP.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertAttention: LRPBertAttention,\n","                         bert.BertIntermediate: LRPBertIntermediate,\n","                         bert.BertOutput: LRPBertOutput}\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        rel_intermediate, rel_attn = self.output.attr_backward(rel_y, eps=eps)\n","        rel_attn += self.intermediate.attr_backward(rel_intermediate, eps=eps)\n","\n","        if self._state[\"crossattention_used\"]:\n","            rel_attn = self.crossattention.attr_backward(rel_attn, eps=eps)\n","        return self.attention.attr_backward(rel_attn, eps=eps)\n","\n","\n","class LRPBertEncoder(BackpropBertEncoder):\n","    \"\"\"\n","    BertEncoder with LRP.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertLayer: LRPBertLayer}\n","\n","    def attr_backward(self, rel_y: HiddenArray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        rel = rel_y\n","        for e in reversed(self.layer):\n","            rel = e.attr_backward(rel, eps=eps)\n","        return rel\n","\n","\n","class LRPBertPooler(LRPBertMixin, BackpropBertPooler):\n","    \"\"\"\n","    BertPooler with LRP.\n","    \"\"\"\n","\n","    def attr_backward(self, rel_y: np.ndarray, eps: float = 0.001) -> \\\n","            HiddenArray:\n","        return self.dense.attr_backward(rel_y, eps=eps)\n","\n","\n","class LRPBertModel(BackpropBertModel):\n","    \"\"\"\n","    BertModel with LRP.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertEmbeddings: LRPBertEmbeddings,\n","                         bert.BertEncoder: LRPBertEncoder,\n","                         bert.BertPooler: LRPBertPooler}\n","\n","    def attr_backward(self, rel_sequence: HiddenArray = None,\n","                      rel_pooled: np.ndarray = None, eps: float = 0.001) -> \\\n","            Tuple[HiddenArray, HiddenArray, HiddenArray]:\n","        assert rel_sequence is not None or rel_pooled is not None\n","\n","        if rel_sequence is None:\n","            rel_sequence = np.zeros(self._state[\"output_shape\"])\n","        if rel_pooled is not None:\n","            rel_first = self.pooler.attr_backward(rel_pooled, eps=eps)\n","            rel_sequence[:, 0] += rel_first\n","\n","        rel_embeddings = self.encoder.attr_backward(rel_sequence, eps=eps)\n","        return self.embeddings.attr_backward(rel_embeddings) + \\\n","               (rel_embeddings,)\n","\n","\n","BBFSC = BackpropBertForSequenceClassification\n","\n","\n","class LRPBertForSequenceClassification(LRPBertMixin, BBFSC):\n","    \"\"\"\n","    A BERT model with a linear decoder.\n","    \"\"\"\n","    _bert_layer_types = {bert.BertModel: LRPBertModel}\n","\n","    def attr_backward(self, rel_y: np.ndarray, eps: float = 0.001) -> \\\n","            Tuple[HiddenArray, HiddenArray, HiddenArray]:\n","        rel_pooled = self.classifier.attr_backward(rel_y, eps=eps)\n","        return self.bert.attr_backward(rel_pooled=rel_pooled, eps=eps)"],"metadata":{"id":"fNwe_ITbZ4Ln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from IPython.core.display import display, HTML\n","from transformers import BertTokenizer\n"],"metadata":{"id":"ebmOPWx3Ltu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install yattag"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVoe5K-naNEQ","executionInfo":{"status":"ok","timestamp":1654782977109,"user_tz":-120,"elapsed":4899,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"34bcefb2-da8b-4d5a-8f8e-bb9794fdc126"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting yattag\n","  Downloading yattag-1.14.0.tar.gz (26 kB)\n","Building wheels for collected packages: yattag\n","  Building wheel for yattag (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yattag: filename=yattag-1.14.0-py3-none-any.whl size=15659 sha256=d1bddb0411205e8634fac7f02f12f33afde9493c867c677999b03477efcdfb30\n","  Stored in directory: /root/.cache/pip/wheels/4d/32/61/f205e276a280e24c3fca996bd956781b2a0fbad498161e53f4\n","Successfully built yattag\n","Installing collected packages: yattag\n","Successfully installed yattag-1.14.0\n"]}]},{"cell_type":"code","source":["from typing import Callable, List, Tuple\n","\n","import matplotlib.pyplot as plt\n","from yattag import Doc\n","\n","\n","def _rescale_score_by_abs(score: float, max_score: float,\n","                          min_score: float) -> float:\n","    \"\"\"\n","    Normalizes an attribution score to the range [0., 1.], where a score\n","    score of 0. is mapped to 0.5.\n","    :param score: An attribution score\n","    :param max_score: The maximum possible attribution score\n","    :param min_score: The minimum possible attribution score\n","    :return: The normalized score\n","    \"\"\"\n","    if -1e-5 < min_score and max_score < 1e-5:\n","        return .5\n","    elif max_score == min_score and min_score < 0:\n","        return 0.\n","    elif max_score == min_score and max_score > 0:\n","        return 1.\n","\n","    top = max(abs(max_score), abs(min_score))\n","    return (score + top) / (2. * top)\n","\n","\n","def _get_rgb(c_tuple: Tuple[float]) -> str:\n","    \"\"\"\n","    Converts a color from a tuple with values in [0., 1.] to RGB format.\n","    :param c_tuple: A color\n","    :return: The color, in RGB format\n","    \"\"\"\n","    return \"#%02x%02x%02x\" % tuple(int(i * 255.) for i in c_tuple[:3])\n","\n","\n","def _span_word(tag: Callable, text: Callable, word: str, score: float,\n","               colormap: Callable):\n","    \"\"\"\n","    Creates an HTML DOM object that contains a word with a background\n","    color representing its attribution score.\n","    :param tag: The tag() method from yattag\n","    :param text: The text() method from yattag\n","    :param word: A word\n","    :param score: The word's attribution score\n","    :param colormap: A matplotlib colormap\n","    :return: None\n","    \"\"\"\n","    bg = colormap(score)\n","    style = \"color:\" + _get_rgb(bg) + \";font-weight:bold;background-color: \" \\\n","                                      \"#ffffff;padding-top: 15px;\" \\\n","                                      \"padding-bottom: 15px;\"\n","    with tag(\"span\", style=style):\n","        text(\" \" + word + \" \")\n","    text(\" \")\n","\n","\n","def html_heatmap(tokens: List[str], scores: List[float],\n","                 cmap_name: str = \"coolwarm\") -> str:\n","    \"\"\"\n","    Constructs a word-level heatmap in HTML format.\n","    :param tokens: A sequence of tokens\n","    :param scores: The attribution score assigned to each token\n","    :param cmap_name: A matplotlib diverging colormap\n","    :return: The heatmap, as HTML code\n","    \"\"\"\n","    colormap = plt.get_cmap(cmap_name)\n","\n","    assert len(tokens) == len(scores)\n","    max_s = max(scores)\n","    min_s = min(scores)\n","\n","    doc, tag, text = Doc().tagtext()\n","\n","    for idx, w in enumerate(tokens):\n","        score = _rescale_score_by_abs(scores[idx], max_s, min_s)\n","        _span_word(tag, text, w, score, colormap)\n","\n","    return doc.getvalue()\n","\n","\n","def latex_heatmap(tokens: List[str], scores: List[float],\n","                  cmap_name: str = \"coolwarm\") -> str:\n","    \"\"\"\n","        Constructs a word-level heatmap in LaTeX format.\n","        :param tokens: A sequence of words\n","        :param scores: The attribution score assigned to each token\n","        :param cmap_name: A matplotlib diverging colormap\n","        :return: The heatmap, as LaTeX code\n","        \"\"\"\n","    colormap = plt.get_cmap(cmap_name)\n","\n","    assert len(tokens) == len(scores)\n","    max_s = max(scores)\n","    min_s = min(scores)\n","\n","    code = \"\"\n","    code_template = \"\\\\textcolor[rgb]{{{},{},{}}}{{\\\\textbf{{{}}}}} \"\n","    for idx, w in enumerate(tokens):\n","        score = _rescale_score_by_abs(scores[idx], max_s, min_s)\n","        r, g, b, _ = colormap(score)\n","        code += code_template.format(r, g, b, w)\n","\n","    return code"],"metadata":{"id":"N57drt4_aSEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n"],"metadata":{"id":"W0LntLdAHABW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv(\"dataforshap.csv\", encoding=\"latin\", header=[0])\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":708},"id":"b0dGxs_3HF_L","executionInfo":{"status":"ok","timestamp":1654782977725,"user_tz":-120,"elapsed":627,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"5e07b399-e1f5-40c6-fd18-ceaee66d8a91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       ï»¿Description     label\n","0   Finnish Bank of +_land reports its operating p...  negative\n","1   Finnish Bank of +_land reports operating profi...  negative\n","2   Finnish electronics contract manufacturer Scan...  negative\n","3   Finnish electronics contract manufacturer Scan...  negative\n","4   Finnish Exel Composites , a technology company...  negative\n","5   Finnish GeoSentric 's net sales decreased to E...  negative\n","6   Finnish Kemira 's net sales EUR decreased to E...  negative\n","7   Finnish plumbing and heating systems supplier ...  negative\n","8   Finnish plumbing and heating systems supplier ...  negative\n","9   Finnish Scanfil , a contract manufacturer and ...  negative\n","10  Finnish Scanfil , a systems supplier and contr...  negative\n","11  Finnish Scanfil , a systems supplier and contr...  negative\n","12  Finnish shipping company Finnlines , of the Gr...  negative\n","13  Cargo traffic fell 1 % year-on-year to 8,561 t...  negative\n","14  Cash flow after investments amounted to EUR45m...  negative\n","15  Coca-Cola was the market leader of manufacture...  negative\n","16  Comparable operating profit decreased to EUR 1...  negative\n","17  Consolidated operating profit from continuing ...  negative\n","18  Consolidated pretax profit decreased by 69.2 %...  negative\n","19  Device volume in the area decreased by 21 % to...  negative\n","20  Diluted earnings per share ( EPS ) declined to...  negative"],"text/html":["\n","  <div id=\"df-9d1f7ac4-ff18-4f25-9271-39136b6737e1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ï»¿Description</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Finnish Bank of +_land reports its operating p...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Finnish Bank of +_land reports operating profi...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Finnish electronics contract manufacturer Scan...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Finnish electronics contract manufacturer Scan...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Finnish Exel Composites , a technology company...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Finnish GeoSentric 's net sales decreased to E...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Finnish Kemira 's net sales EUR decreased to E...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Finnish plumbing and heating systems supplier ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Finnish plumbing and heating systems supplier ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Finnish Scanfil , a contract manufacturer and ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Finnish Scanfil , a systems supplier and contr...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Finnish Scanfil , a systems supplier and contr...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Finnish shipping company Finnlines , of the Gr...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Cargo traffic fell 1 % year-on-year to 8,561 t...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Cash flow after investments amounted to EUR45m...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Coca-Cola was the market leader of manufacture...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Comparable operating profit decreased to EUR 1...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Consolidated operating profit from continuing ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Consolidated pretax profit decreased by 69.2 %...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Device volume in the area decreased by 21 % to...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Diluted earnings per share ( EPS ) declined to...</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d1f7ac4-ff18-4f25-9271-39136b6737e1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9d1f7ac4-ff18-4f25-9271-39136b6737e1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9d1f7ac4-ff18-4f25-9271-39136b6737e1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["data=df['ï»¿Description']\n","labels=df['label']"],"metadata":{"id":"88p5ze97HKGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jyx3Y0jWHLMK","executionInfo":{"status":"ok","timestamp":1654782977728,"user_tz":-120,"elapsed":12,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"ef4e6d2f-f912-4d31-e31e-f8bb721b6299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finnish electronics contract manufacturer Scanfil reports net sales of EUR 58.9 mn in the second quarter of 2007 , down from EUR 62.4 mn a year earlier .\n"]}]},{"cell_type":"code","source":["print(\"Loading model...\")\n","config_path = \"bert-sst-config.pt\"\n","state_dict_path = \"bert-sst.pt\"\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = LRPBertForSequenceClassification(torch.load(config_path))\n","model.load_state_dict(torch.load(state_dict_path))\n","model.eval()\n","print(\"Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1pMeSBmaUSN","executionInfo":{"status":"ok","timestamp":1654782989397,"user_tz":-120,"elapsed":11679,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"389eb2d1-8e34-497d-927b-d3de76c8fdd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Done.\n"]}]},{"cell_type":"code","source":["text = \"Finnish electronics contract manufacturer Scanfil reports net sales of EUR 58.9 mn in the second quarter of 2007 , down from EUR 62.4 mn a year earlier.\""],"metadata":{"id":"acP98nifaZ_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","logits = model(**inputs).logits.squeeze()\n","    \n","classes = [\"<unk>\", \"negative\", \"positive\", \"neutral\"]\n","print(\"Logit Scores:\")\n","for c, score in zip(classes, logits):\n","    print(\"{}: {}\".format(c, score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fxtst874adMp","executionInfo":{"status":"ok","timestamp":1654783075268,"user_tz":-120,"elapsed":298,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"aa311498-bf6b-49f7-b3f6-9ce338f6883d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit Scores:\n","<unk>: -3.24169921875\n","negative: 2.122391939163208\n","positive: -0.27142736315727234\n","neutral: 0.7424658536911011\n"]}]},{"cell_type":"code","source":["inputs = tokenizer(text, return_tensors=\"pt\")\n","model.attr()\n","output = model(**inputs)\n","\n","print(\"Attr Forward Pass Output:\")\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vs71zgZafNE","executionInfo":{"status":"ok","timestamp":1654783079621,"user_tz":-120,"elapsed":295,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"f929976d-530e-42a2-aef0-ecc0e368659f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:700: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n"]},{"output_type":"stream","name":"stdout","text":["Attr Forward Pass Output:\n","[[-3.2416987   2.122393   -0.27142838  0.7424659 ]]\n"]}]},{"cell_type":"code","source":["tokens = tokenizer.tokenize(text)\n","rel_y = np.zeros(output.shape)\n","print(rel_y)\n","rel_y[:, 1] = output[:, 1]\n","print(rel_y[:, 1])\n","print(output[:, 1])\n","print(rel_y)\n","rel_word, rel_pos, rel_type, rel_embed = model.attr_backward(rel_y, eps=.1)\n","rel_word = np.sum(rel_word[0, 1:-1], -1)\n","rel_pos = np.sum(rel_pos[0, 1:-1], -1)\n","rel_type = np.sum(rel_type[0, 1:-1], -1)\n","rel_embed = np.sum(rel_embed[0, 1:-1], -1)\n","\n","print(\"LRP Scores:\")\n","for t, s in zip(tokens, rel_embed):\n","    print(t, s, sep=\": \")\n","    \n","print(\"Relevance of word embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_word))))\n","\n","print(\"Relevance of positional embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_pos))))\n","\n","print(\"Relevance of type embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_type))))\n","\n","print(\"Relevance of combined embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_embed))))"],"metadata":{"id":"gc2kj-UYahKg","colab":{"base_uri":"https://localhost:8080/","height":890},"executionInfo":{"status":"ok","timestamp":1654783116134,"user_tz":-120,"elapsed":19633,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"fe484ebd-3ca8-459f-8ff8-6b1f9ce4b234"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]]\n","[2.12239289]\n","[2.122393]\n","[[0.         2.12239289 0.         0.        ]]\n","LRP Scores:\n","finnish: -0.14522554551076988\n","electronics: 0.14776049915683992\n","contract: 0.013330546950413753\n","manufacturer: -0.31128494420981334\n","scan: -0.4742611466990175\n","##fi: -0.5491764659228147\n","##l: -0.22518640598134237\n","reports: -0.30848162134053125\n","net: -0.18273150199133273\n","sales: -0.18608534832103782\n","of: -0.2888951537757459\n","eu: -0.03957476802482962\n","##r: 0.12110666352457783\n","58: -0.11890675666491485\n",".: -0.1548909514399779\n","9: -0.20034812141119068\n","mn: 0.17646247633517598\n","in: -0.02391393774897143\n","the: -0.026808201459421997\n","second: 0.02836921116950749\n","quarter: -0.03604754526137735\n","of: -0.06196085837188646\n","2007: 0.02426563322531921\n",",: -0.027812935087736475\n","down: -0.1088091468826214\n","from: -0.0351232421480627\n","eu: -1.1194270572308371\n","##r: -0.2577217621688427\n","62: -0.7320260788922421\n",".: 0.2468131365015523\n","4: -0.33742821748024615\n","mn: 0.0402080993610075\n","a: -0.3353593228835676\n","year: 0.3245448321786031\n","earlier: 0.2300795340996656\n",".: -0.6243938038499063\n","Relevance of word embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#b0cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#698bef;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#86a9fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#87aafc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#9bbbfe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#6b8df0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#bbd1f7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#a6c3fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#bcd1f6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#a3c1fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#8daffd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#f3c5af;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#d2dae7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#c7d6f0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#ead3c7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#c2d4f3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#ecd1c3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#5c7be5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#a2c0fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#b0cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f7ad8f;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#e7d6cd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#8baefd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of positional embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#a6c3fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#f7b598;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#f3c6b0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#abc7fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#e2d9d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#e6d7cf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#bcd1f6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#97b8fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#ecd2c4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#edd0c1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#9dbdfe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#b6cef9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#f6bda4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#ead3c7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f4c2aa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#aec9fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of type embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#4b64d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#e9795e;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#f39678;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#efcebc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#f6ba9f;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#f2c8b3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#ef896c;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#f08b6d;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#f3c5af;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#82a5fb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#f1cbb8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#f1cab6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#b4cdfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#87aafc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#edd0c1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#a0bffe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of combined embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#ecd2c4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#b3ccfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#99bafe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#8eb1fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#c0d3f5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#b3ccfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#c6d6f1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#b6cef9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#c3d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#bbd1f7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#7194f4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#f2c7b2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#afcafb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#afcafb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f6bda4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#82a5fb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer2 = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n","cl_path = '/content/drive/MyDrive/UNCHANGEDweights/L0/classifier_model/finbert-sentiment'\n","model2 = AutoModelForSequenceClassification.from_pretrained(cl_path, cache_dir=None, num_labels=3)\n"],"metadata":{"id":"ffTzu3n7dQY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2.eval()\n","inputs = tokenizer2(text, return_tensors=\"pt\")\n","logits = model2(**inputs).logits.squeeze()\n","    \n","classes = [\"positive\", \"negative\", \"neutral\", \"unkown\"]\n","print(\"Logit Scores:\")\n","for c, score in zip(classes, logits):\n","    print(\"{}: {}\".format(c, score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5Yhuc7SgQKu","executionInfo":{"status":"ok","timestamp":1654785905545,"user_tz":-120,"elapsed":37,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"dad9a2d8-3dfb-4ef0-f53a-80e88a73cb41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit Scores:\n","positive: -1.6475794315338135\n","negative: 2.933547258377075\n","neutral: -2.043841600418091\n"]}]},{"cell_type":"code","source":["inputs = tokenizer2(text, return_tensors=\"pt\")\n","output = model2(**inputs)\n","\n","print(\"Attr Forward Pass Output:\")\n","\n","x=output[0]\n","a=x.tolist()\n","# print(a[0][0])\n","b=a[0][1]\n","print(b)\n","# b = ''.join(str(a).split(','))\n","# print(b)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJ9nS0UFgie1","executionInfo":{"status":"ok","timestamp":1654785905546,"user_tz":-120,"elapsed":24,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"5a002fe4-ec46-4e1b-b833-3b0e4d4fac4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attr Forward Pass Output:\n","2.933547258377075\n"]}]},{"cell_type":"code","source":["print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82MZ0ikQvRZW","executionInfo":{"status":"ok","timestamp":1654785905546,"user_tz":-120,"elapsed":20,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"7eade105-59fa-4298-efd7-e72df04ac836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.933547258377075\n"]}]},{"cell_type":"code","source":["classes = [\"positive\", \"negative\",  \"neutral\", \"unkown\"]\n","print(\"Logit Scores:\")\n","for c, score in zip(classes, logits):\n","    print(\"{}: {}\".format(c, score))\n","tokens = tokenizer.tokenize(text)\n","rel_y =np.zeros((1, 4))\n","rel_y[0][1]=2.933547258377075\n","\n","print(rel_y)\n","\n","rel_word, rel_pos, rel_type, rel_embed = model.attr_backward(rel_y, eps=.1)\n","rel_word = np.sum(rel_word[0, 1:-1], -1)\n","rel_pos = np.sum(rel_pos[0, 1:-1], -1)\n","rel_type = np.sum(rel_type[0, 1:-1], -1)\n","rel_embed = np.sum(rel_embed[0, 1:-1], -1)\n","\n","print(\"LRP Scores:\")\n","for t, s in zip(tokens, rel_embed):\n","    print(t, s, sep=\": \")\n","    \n","print(\"Relevance of word embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_word))))\n","\n","print(\"Relevance of positional embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_pos))))\n","\n","print(\"Relevance of type embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_type))))\n","\n","print(\"Relevance of combined embeddings:\")\n","display(HTML(html_heatmap(tokens, list(rel_embed))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":907},"id":"cjqJ7zDth2Vx","executionInfo":{"status":"ok","timestamp":1654785925554,"user_tz":-120,"elapsed":20025,"user":{"displayName":"Fatjon Huseini","userId":"04797314500465995857"}},"outputId":"a57cc9a1-dc72-4401-8380-da293fadeb68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit Scores:\n","positive: -1.6475794315338135\n","negative: 2.933547258377075\n","neutral: -2.043841600418091\n","[[0.         2.93354726 0.         0.        ]]\n","LRP Scores:\n","finnish: -0.20072909324053648\n","electronics: 0.20423287726828998\n","contract: 0.018425330008889337\n","manufacturer: -0.4302545008242597\n","scan: -0.6555183497592796\n","##fi: -0.7590654498560926\n","##l: -0.3112500829227602\n","reports: -0.4263797799159477\n","net: -0.2525693986701637\n","sales: -0.2572050468287713\n","of: -0.3993075877596922\n","eu: -0.05469979315890808\n","##r: 0.16739224954656962\n","58: -0.1643515633670075\n",".: -0.2140885071174003\n","9: -0.27691888917938956\n","mn: 0.2439044228834571\n","in: -0.033053572105918624\n","the: -0.037053990408046786\n","second: 0.03921160023173967\n","quarter: -0.04982450607029694\n","of: -0.08564159200539354\n","2007: 0.033539681583530725\n",",: -0.038442721773802875\n","down: -0.15039476225211035\n","from: -0.04854694484535893\n","eu: -1.547259315546554\n","##r: -0.3562200812987729\n","62: -1.011798100173558\n",".: 0.3411423032742916\n","4: -0.4663894350705497\n","mn: 0.05557517651095434\n","a: -0.4635298325471726\n","year: 0.4485821667943548\n","earlier: 0.31801330881973766\n",".: -0.8630299967611876\n","Relevance of word embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#b0cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#698bef;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#86a9fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#87aafc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#9bbbfe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#6b8df0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#bbd1f7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#a6c3fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#bcd1f6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#a3c1fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#8daffd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#f3c5af;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#d2dae7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#c7d6f0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#ead3c7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#c2d4f3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#ecd1c3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#5c7be5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#a2c0fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#b0cbfb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f7ad8f;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#e7d6cd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#8baefd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of positional embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#a6c3fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#f7b598;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#f3c6b0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#dddcdb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#abc7fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#e2d9d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#e6d7cf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#bcd1f6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d0dae9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#97b8fe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#e3d9d3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#ecd2c4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#edd0c1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#e4d8d1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#d7dbe2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#9dbdfe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#b6cef9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#f6bda4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#ead3c7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f4c2aa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#efcdbb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#aec9fc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of type embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#4b64d4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#e9795e;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#f39678;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#efcebc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#f6ba9f;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#f2c8b3;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#ef896c;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#ebd3c6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#f08b6d;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#f3c5af;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#82a5fb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#f1cbb8;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d3dbe6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#e8d5ca;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#f1cab6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#b4cdfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#b30326;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#cdd9ec;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#87aafc;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#edd0c1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#dbdcde;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#a0bffe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#e5d8d0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Relevance of combined embeddings:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style=\"color:#cad8ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> finnish </span> <span style=\"color:#ecd2c4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> electronics </span> <span style=\"color:#dedbda;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> contract </span> <span style=\"color:#b3ccfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> manufacturer </span> <span style=\"color:#99bafe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> scan </span> <span style=\"color:#8eb1fd;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##fi </span> <span style=\"color:#c0d3f5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##l </span> <span style=\"color:#b3ccfa;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> reports </span> <span style=\"color:#c6d6f1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> net </span> <span style=\"color:#c5d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> sales </span> <span style=\"color:#b6cef9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#e9d4c9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#ced9eb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 58 </span> <span style=\"color:#c9d7ee;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#c3d5f2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 9 </span> <span style=\"color:#eecfbe;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#dadcdf;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> in </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> the </span> <span style=\"color:#e0dad7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> second </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> quarter </span> <span style=\"color:#d5dbe5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> of </span> <span style=\"color:#dfdbd9;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 2007 </span> <span style=\"color:#d9dce0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> , </span> <span style=\"color:#cfd9ea;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> down </span> <span style=\"color:#d8dbe1;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> from </span> <span style=\"color:#3a4cc0;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> eu </span> <span style=\"color:#bbd1f7;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> ##r </span> <span style=\"color:#7194f4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 62 </span> <span style=\"color:#f2c7b2;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> <span style=\"color:#afcafb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> 4 </span> <span style=\"color:#e1dad6;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> mn </span> <span style=\"color:#afcafb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> a </span> <span style=\"color:#f6bda4;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> year </span> <span style=\"color:#f2c9b5;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> earlier </span> <span style=\"color:#82a5fb;font-weight:bold;background-color: #ffffff;padding-top: 15px;padding-bottom: 15px;\"> . </span> "]},"metadata":{}}]}]}